{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FT-ICR analysis - PART 2\n",
    "\n",
    "Created on 01 March 2020 for the Pentatrap experiment\n",
    "\n",
    "@author: Menno Door<br>\n",
    "@contact: door+fticr@mpi-k.de<br>\n",
    "@license: MIT license\n",
    " \n",
    "### Refenences :\n",
    "\n",
    "For references reguarding the theory behind ft-icr detection and analysis methods, please check the references.md file.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This part of the analysis requires pre-analysed data of PART 1, meaning fitted frequencies and phase data. This part determines frequencies from phase data, calculating the free cyclotron frequency, filters/masks data and calculats the ratios using multiple analysis methods. The pre-analysed data has to be given in csv data files in one folder.\n",
    "\n",
    "### Contents:\n",
    "- step 0: loading data from PART1\n",
    "- step 4: N determination (\\\"phase unwrap\\\")\n",
    "- step 5: Frequency determination from phase (currently only calc nu_p from phase)\n",
    "- step 6: calc nu_c (6.1 single phase sideband analysis... this we will get rid of)\n",
    "- step 7: filter frequency results\n",
    "- step 8: determine R using naive method\n",
    "- step 9: determine R using interpolation method\n",
    "- step 10: determine R using polynomial method\n",
    "- step 11: determine R using cancellation-naive method\n",
    "- step 12: determine R using cancellation-interpolation method\n",
    "- step 13: determine R using cancellation-polynomial method\n",
    "- step -1: compare results\n",
    "\n",
    "### README & general hints\n",
    "\n",
    "This is a short summary of steps you might want to check and problems you might run into.\\n\",\n",
    "\n",
    "    0) When you start a new analysis, definitly restart the kernel of the notebook and clear the outputs. Use the \\\"fast-forward\\\"-button (two arrow-head showing right) or go to Kernel -> Restart & Clear Output\n",
    "    1) Check the input parameters: Two cells below from this one there is a cell tagged \\\"parameters\\\" which lists the main variables and information needed for the analysis. This includes folders or settings for fits or other stuff. Please check it out to see what you may need to cheange there, each parameter is explained. There can be also a settings file in a measurement folder where people saved their settings (e.g. post_unwrap is set to True and its probably needed for this measurement...). These settings are always overwritten with the settings given here, so if you want to just make settings from the file, comment out the parameter in the settings dict here. \n",
    "    2) Please checkout the overview in Step 0 (loading data), how stabe is the axial frequency, the phases... Also there is an input data filter, typically given in the filter settings file (see parameter list), so if you see already in these plots that some data in the end is just random noise (phase), adjust the filter settings file and dont even load the data, that makes the analysis much easier.\n",
    "    3) Check the last table in the output of Step 4 (N determination). Check if there is a wrongly determined N by checking the nu_p and end_phase. If its a q/m doublet the nu_p should be the same ish, if the phases are different by roughly 2 pi (phases are unwraped, so its possible to have 2pi), there should be a difference in N by 1. If you dont have a q/m doublet you should estimate by hand what a +-1 in N would change for your ratio and check in the end if everything checks out (compared to expected ratio).\n",
    "    4) Check the filter plot in the end of Step7, maybe some values have been auto-filtered, which you might have kept or the other way around. Especially for main cycles with more false data then good, there could be problems. Filtered values are just masked, all saved files still include them.\n",
    "    5) There are multiple ways to determine R from the data, all results should end up with the same value, the errors can be very different though.\n",
    "    6) At the end far down, the different results will be compared.\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "The following code was written in Python 3.7/3.8. The required libraries are listed below with a rough description for their task in the code.\n",
    "\n",
    "    pandas (data organisation, calculation and visualization)\n",
    "    numpy (calculation)\n",
    "    matplotlib (plotting)\n",
    "    scipy (chi square fitting)\n",
    "    jupyter (Python notebook environment)\n",
    "    ipywidgets (https://ipywidgets.readthedocs.io/en/latest/user_install.html)\n",
    "    plotly (plotting, https://github.com/plotly/plotly.py#installation)\n",
    "    qgrid (data visualization, https://github.com/quantopian/qgrid#installation)\n",
    "    \n",
    "### TODO:\n",
    "\n",
    "- mean reference phase is somehow bugged, either its a trap 3 thing or something due to high value reference phases, resulting in highly negative measurement phases... dont known, somethings messud up there, but I dont get where...\n",
    "- change to plotly at some of the smaller plots where it makes sense.\n",
    "- step? (test): normallity test function\n",
    "- step8 (polyfit): SettingWithCopyWarining ... someone cares?\n",
    "- step8 (polyfit): add another error esitmation for the fit: residuals + Kolmogorow-Smirnow-Test if the residuals are Gau√ü-distributed\n",
    "\n",
    "### PROBLEMS:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libs\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import os, json, time\n",
    "from pprint import pprint\n",
    "\n",
    "# math and data\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import constants\n",
    "import pandas as pd\n",
    "\n",
    "# visualization\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "display(HTML(\"<style>div.output_scroll { height: 300em; }</style>\"))\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "try:\n",
    "    %matplotlib inline\n",
    "    show_tag = True\n",
    "except:\n",
    "    show_tag = False\n",
    "    print(\"no ipython backend\")\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tck\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "import qgrid\n",
    "\n",
    "py.init_notebook_mode()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 4)\n",
    "\n",
    "# this package\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from fticr_toolkit import *\n",
    "from fticr_toolkit import ideal_trap_physics as itp\n",
    "\n",
    "### WARNING !!! ###\n",
    "# this will remove the warnings from pandas regarding assignments of copies of DataFrames to the other/same DataFrames. This\n",
    "# is fine regarding the features implemented, but if you change stuff and its not working out as you think, you should \n",
    "# enable this again, to check if this is the problem.\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# define floating points on display, this will be changed later to higher precision\n",
    "pd.options.display.float_format = '{: .3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "##   P A R A M E T E R   L I S T   ( this is especially used for batch processing using the papermill package but its also nice to have thesse parameters all at one place )\n",
    "\n",
    "measurement_folder = \"E:\\\\local_analysis\\\\Yb_CHAR\\\\176Yb44+_176Yb43+_176Yb44+_4\"\n",
    "measurement_folder = \"E:\\\\\\\\local_analysis\\\\Yb\\\\172Yb43+_176Yb44+_172Yb43+\"\n",
    "measurement_folder = \"E:\\\\local_analysis\\\\Yb168172\\\\172Yb43+_168Yb42+_172Yb43+_3\"\n",
    "measurement_folder = \"Z:\\\\DATA_for_ANALYSIS\\\\analysis_Menno\\\\Yb174172\\\\174Yb42+_172Yb42+_174Yb42+_var_10\"\n",
    "measurement_folder = \"G:\\\\Yb\\\\172174_var3_opti\\\\174Yb42+_172Yb42+_174Yb42+_var3_48_opti\"\n",
    "measurement_folder = \"G:\\\\Yb\\\\172174_var1\\\\174Yb42+_172Yb42+_174Yb42+_var_6\"\n",
    "measurement_folder = \"G:\\\\Yb\\\\172176_var\\\\176Yb42+_172Yb42+_176Yb42+_var_11\"\n",
    "#measurement_folder = \"G:\\\\Yb\\\\172170_var\\\\172Yb42+_170Yb42+_172Yb42+_var_8\"\n",
    "measurement_folder = \"G:\\\\Yb\\\\17241+42+_binding_var\\\\172Yb42+_172Yb41+_172Yb42+_var_1\"\n",
    "#measurement_folder = \"G:\\\\Yb\\\\17242+43+_binding_var\\\\172Yb42+_172Yb43+_172Yb42+_var_3\"\n",
    "measurement_folder = \"G:\\\\Yb_dualPnPtest\\\\172Yb43+_dual_num_nup_test_3\"\n",
    "measurement_folder = \"G:\\\\Yb\\\\17241+42+_binding_var_dualPnP\\\\172Yb42+_172Yb41+_172Yb42+_dual_8\"\n",
    "\n",
    "#measurement_folder = \"Z:/DATA_for_ANALYSIS/analysis_Menno/20_Re_Os_ratio_cancellation/187Re29+_187Os29+_187Re29+_12\"\n",
    "\n",
    "input_folder = \"./part1_data/\" # the measurements subfolder where the pre-analysed data was saved (output of PART 1: spectra fits and/or phases) \n",
    "output_folder = \"./results/\" # subfolder where the results of this analysis should go\n",
    "# analysis parameters are loaded from a (more static and measurement specific) settings file ('analysis_settings.json') inside the measurement folder\n",
    "# for testing and batch processing the parameters can be adjusted below\n",
    "settings = {\n",
    "    # The filter settings are used to remove some data, which is nice if e.g. the ion was lost at some point and you dont want to see crappy random data. Either directly supply a pandas dset (more usefull for papermill batches) or rather\n",
    "    # create a csv file as in the examples and just assign None to this variable, it will use the csv file then. \n",
    "    \"grouping\": [6],\n",
    "    \"filter_settings\": None, # pandas dset with mc, trap, position, min_cycle, max_cycle; None: try to get filter settings from loaded data; False: no filter applied\n",
    "    #\"filter_settings\": pd.DataFrame(\n",
    "    #    columns=[\"mcycle\", \"trap\", \"position\", \"min_cycle\", \"max_cycle\"],\n",
    "    #    data=[[1,2,\"position_1\", 0, 26],[1,2,\"position_2\", 0, 26],[1,3,\"position_1\", 0, 26],[1,3,\"position_2\", 0, 26]]\n",
    "    #          ),\n",
    "    \"post_unwrap\": False, # if this is True, the analysis will automatically choose the post-unwrap (the pre-unwrap of the next main cycle) for nu_p frequency determination. Maybe the whole post-unwrap part is commented out in step 5, please check.\n",
    "    \"unwrap_range\": 6,\n",
    "    \"mean_ref_phase\": True, # BUGGED!!!! DONT USE! if True, the reference phase will be averaged and the same value substracted for all long phases (not in Ndet), otherwise every single measured reference phase is substraced from the respective long phase\n",
    "    \"phase_error_undrift\": True, # makes everything worse supprisingly\n",
    "    \"phase_filter\": True, # filter measured phases by 3 sigma filter inside subcycle\n",
    "    \"single_axial\": True, # use single spec no average axial data\n",
    "    \"average\": False, # averaging subcycle data to match average_idx from part 1\n",
    "    \"nu_z_from_config\": False, # default: False, if float, the config nu_z will be taken and the float value is used as the error on the config nu_z\n",
    "    \"fill_nu_z_from_config\": False, #0.08, # default: False, if float, the config nu_z will be taken if the measured nu_z is off by hardcoded 100 Hz (which also includes no value at all) and the float value is used as the error on the config nu_z\n",
    "    \"sideband\": False, # use sideband relation to calculate nu_c\n",
    "    \"nu_m2_from_theory\": False, # use the magnetron freq of one position to calculate the magnetron of the other position, keeping the difference right, common offset doesn't matter\n",
    "    \"polydegrees\": 'auto', # number of degree of the polynom fit\n",
    "    \"polygrouping\": 'auto', # group sizes for the polynom fit\n",
    "    \"poly_mode\": \"curvefit\", # routine for polynom fitting\n",
    "    \"poly_criterion\": \"AICc\", # criterion for best model/poly-degree\n",
    "    \"invert\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Load data\n",
    "\n",
    "Please supply the measurement folder and filenames to the csv datasets of pre-analysed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data, meas_config, results_dir = data_conversion.load_data(measurement_folder=measurement_folder,\n",
    "                                              input_data=input_folder, # inside measurement folder\n",
    "                                              output_folder=output_folder, # inside measurement folder\n",
    "                                              measurement_script = \"unwrap_n_measure_repeat\") # inside measurement folder\n",
    "\n",
    "try:\n",
    "    settings = data_conversion.load_settings(measurement_folder, settings)\n",
    "except:\n",
    "    print(\"no settings from file!\")\n",
    "print(\"settings\", settings)\n",
    "#print(\"data\", data.keys())\n",
    "#print(\"invert? (default=True)\", settings['invert'])\n",
    "\n",
    "try:\n",
    "    data = data_conversion.input_filter(data, settings=settings[\"filter_settings\"])\n",
    "except:\n",
    "    #raise\n",
    "    print(\"no input filter\")\n",
    "\n",
    "# define local namespace names for the data, just easier\n",
    "nu_p_unwrap = data[\"pre_unwrap_phase\"]\n",
    "nu_m_unwrap = data[\"pre_unwrap_phase2\"]\n",
    "nu_p_phases = data[\"phase_data\"]\n",
    "nu_m_phases = data[\"phase2_data\"]\n",
    "axial_data = data[\"axial_data\"]\n",
    "if settings.get(\"single_axial\", False):\n",
    "    axial_data = data[\"axial_data_single\"]\n",
    "\n",
    "# get the variables/lists we will be looping over...\n",
    "mcs = nu_p_unwrap.mcycle.unique()\n",
    "print(\" >>> MAIN CYCLES: \", mcs)\n",
    "super_cycle_list = []\n",
    "for mc in mcs:\n",
    "    subset = axial_data[axial_data['mcycle']==mc]\n",
    "    subcycles = subset.cycle.unique()\n",
    "    super_cycle_list.append(list(subcycles))\n",
    "print(\" >>> SUB CYCLES: \", super_cycle_list, \"\\n\")\n",
    "positions = axial_data.position.unique()\n",
    "print(positions)\n",
    "start_position = meas_config[\"start_position\"]\n",
    "other_position = positions[0]\n",
    "if start_position == other_position:\n",
    "    try:\n",
    "        other_position = positions[1]\n",
    "    except:\n",
    "        other_position = start_position\n",
    "    \n",
    "print(\" >>> POSITIONS: \", positions, \" - > start at\", start_position)\n",
    "traps = nu_p_unwrap.trap.unique()\n",
    "print(\" >>> TRAPS:     \", traps)\n",
    "\n",
    "if show_tag:\n",
    "    # ...and just roughly check the data...\n",
    "    print(\" >>> AXIAL DATA >>> \")\n",
    "    #axial_data = axial_data[ (axial_data['mcycle']==1) & (axial_data['cycle'] >= 0) ]\n",
    "    fig = px.scatter(axial_data, x=\"time\", y=\"nu_z\", error_y=\"dnu_z\", facet_row=\"position\", facet_col=\"trap\", hover_data=['mcycle', 'cycle', 'position'])\n",
    "    #fig = px.scatter(axial_data, x=\"time\", y=\"nu_z\", error_y=\"dnu_z\", facet_col=\"trap\", color=\"position\", hover_data=['mcycle', 'cycle', 'position'])\n",
    "    fig.update_yaxes(matches=None, showticklabels=True)\n",
    "    fig.show()\n",
    "    #display(axial_data)\n",
    "    \n",
    "    try:\n",
    "        fig = px.scatter(axial_data, x=\"time\", y=\"Q\", error_y=\"dQ\", facet_col=\"trap\", color='position', hover_data=['mcycle', 'cycle', 'position'])\n",
    "        fig.update_yaxes(matches=None, showticklabels=True)\n",
    "        fig.show()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        fig = px.scatter(axial_data, x=\"time\", y=\"nu_res\", error_y=\"dnu_res\", facet_col=\"trap\", facet_row='position', hover_data=['mcycle', 'cycle', 'position'])\n",
    "        fig.update_yaxes(matches=None, showticklabels=True)\n",
    "        fig.show()\n",
    "        for gname, grp in axial_data.groupby([\"trap\", \"position\"]):\n",
    "            print(gname, statistics.complete_mean_and_error(grp.nu_res.to_numpy(), grp.dnu_res.to_numpy()), np.std(grp.nu_res.to_numpy()))\n",
    "    except:\n",
    "        raise\n",
    "        \n",
    "\n",
    "    try:\n",
    "        axial_data[\"offnu_res\"] = axial_data[\"nu_z\"] - axial_data[\"nu_res\"]\n",
    "        axial_data[\"doffnu_res\"] = np.sqrt(axial_data[\"dnu_z\"].to_numpy()**2 + axial_data[\"dnu_res\"].to_numpy()**2)\n",
    "        fig = px.scatter(axial_data, x=\"time\", y=\"offnu_res\", error_y=\"doffnu_res\", facet_col=\"trap\", facet_row='position', hover_data=['mcycle', 'cycle', 'position'])\n",
    "        fig.update_yaxes(matches=None, showticklabels=True)\n",
    "        fig.show()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \"\"\" Allen Deviation for axial\n",
    "    for trap, data in axial_data.groupby(\"trap\"):\n",
    "        data = data.sort_values(\"time\")\n",
    "        data['epoch'] = data[\"time\"].astype(\"int64\")//1e9\n",
    "        data['seconds'] = data['epoch'] - data['epoch'].min()\n",
    "        t = data.seconds.to_numpy()/60\n",
    "        y = data.nu_z.to_numpy()\n",
    "        y = y/np.mean(y)\n",
    "        visualization.allanvariance(y, t, plot=True)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\" >>> PHASE DATA >>> \")\n",
    "    nu_p_phases[\"acc_time\"] = nu_p_phases[\"acc_time\"].astype(str)\n",
    "    #fig = px.scatter(nu_p_phases, x=\"time\", y=\"phase\", facet_row=\"trap\", facet_col=\"position\", color=\"acc_time\", hover_data=['mcycle', 'cycle', 'position'])\n",
    "    fig = px.scatter(nu_p_phases, x=\"time\", y=\"phase\", facet_row=\"position\", facet_col=\"trap\", color=\"acc_time\", hover_data=['mcycle', 'cycle', 'position'])\n",
    "    #fig = px.scatter(nu_p_phases, x=\"time\", y=\"phase\", facet_col=\"trap\", color=\"position\", hover_data=['mcycle', 'cycle', 'position'])\n",
    "    fig.show()\n",
    "    nu_p_phases[\"acc_time\"] = nu_p_phases[\"acc_time\"].astype(float)\n",
    "\n",
    "    \n",
    "    print(\" >>> PHASE DATA >>> \")\n",
    "    nu_m_phases[\"acc_time\"] = nu_m_phases[\"acc_time\"].astype(str)\n",
    "    #fig = px.scatter(nu_p_phases, x=\"time\", y=\"phase\", facet_row=\"trap\", facet_col=\"position\", color=\"acc_time\", hover_data=['mcycle', 'cycle', 'position'])\n",
    "    fig = px.scatter(nu_m_phases, x=\"time\", y=\"phase\", facet_row=\"position\", facet_col=\"trap\", color=\"acc_time\", hover_data=['mcycle', 'cycle', 'position'])\n",
    "    #fig = px.scatter(nu_p_phases, x=\"time\", y=\"phase\", facet_col=\"trap\", color=\"position\", hover_data=['mcycle', 'cycle', 'position'])\n",
    "    fig.show()\n",
    "    nu_m_phases[\"acc_time\"] = nu_m_phases[\"acc_time\"].astype(float)\n",
    "    \n",
    "    print(\" >>> PHASE DIFFERENCE START POSITION - OTHER POSITION >>> \")\n",
    "    try:\n",
    "        nu_p_phases[\"acc_time\"] = nu_p_phases[\"acc_time\"].astype(str)\n",
    "        posA = nu_p_phases[nu_p_phases[\"position\"] == start_position]\n",
    "        posB = nu_p_phases[nu_p_phases[\"position\"] != start_position]\n",
    "        posA[\"phase_diff\"] = posA.phase.to_numpy() - posB.phase.to_numpy()\n",
    "        fig = px.scatter(posA, x=\"time\", y=\"phase_diff\", facet_row=\"trap\", facet_col=\"position\", color=\"acc_time\", hover_data=['mcycle', 'cycle', 'position'])\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(' DIFF only works with equal size phase data')\n",
    "    finally:\n",
    "        nu_p_phases[\"acc_time\"] = nu_p_phases[\"acc_time\"].astype(float)\n",
    "        \n",
    "    print(\" >>> UNWRAP DATA >>> \")\n",
    "    nu_p_unwrap[\"acc_time\"] = nu_p_unwrap[\"acc_time\"].astype(str)\n",
    "    fig = px.scatter(nu_p_unwrap, x=\"time\", y=\"phase\", facet_col=\"trap\", facet_row=\"position\", color=\"acc_time\", hover_data=['mcycle', 'cycle', 'position'])\n",
    "    fig.show()\n",
    "    nu_p_unwrap[\"acc_time\"] = nu_p_unwrap[\"acc_time\"].astype(float)\n",
    "    \n",
    "    nu_m_unwrap[\"acc_time\"] = nu_m_unwrap[\"acc_time\"].astype(str)\n",
    "    fig = px.scatter(nu_m_unwrap, x=\"time\", y=\"phase\", facet_col=\"trap\", facet_row=\"position\", color=\"acc_time\", hover_data=['mcycle', 'cycle', 'position'])\n",
    "    fig.show()\n",
    "    nu_m_unwrap[\"acc_time\"] = nu_m_unwrap[\"acc_time\"].astype(float)\n",
    "    \n",
    "    \"\"\"\n",
    "    # trap2\n",
    "    t2_phases = nu_p_unwrap[ (nu_p_unwrap['trap']==2) & (nu_p_unwrap['position']=='position_2') ]\n",
    "    #display(t2_phases)\n",
    "    accs = []\n",
    "    pstds = []\n",
    "        accs.append( float(group.acc_time.unique()) )\n",
    "        pstds.append( np.std( np.unwrap( group.phase.to_numpy() ) ) /2/np.pi*360 )\n",
    "    plt.scatter(accs, pstds, marker='o')\n",
    "    plt.show()\n",
    "    \n",
    "    t3_phases = nu_p_unwrap[ (nu_p_unwrap['trap']==3) & (nu_p_unwrap['position']=='position_2') ]\n",
    "    #display(t3_phases)\n",
    "    accs = []\n",
    "    pstds = []\n",
    "    for name, group in t3_phases.groupby(['mcycle', 'acc_time']):\n",
    "        accs.append( float(group.acc_time.unique()) )\n",
    "        pstds.append( np.std( np.unwrap( group.phase.to_numpy() ) ) /2/np.pi*360 )\n",
    "    plt.scatter(accs, pstds, marker='o')\n",
    "    plt.show()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: N determination\n",
    "\n",
    "The unwrap data (in this case just for the nu_p phase measurement) is used to calculate the total N of osciallations during the phase accumulation time in the later measurement\n",
    "\n",
    "The method n_determination.fit_N will unwrap the phases for each acc_time, average, substract the reference phases from all the other measured phases and then determine the N by calculating Ns for a 1 Hz range around the guessed frequency and searches for the minimum. A plot of the Ns and the found minimum will be plotted to check the results.\n",
    "\n",
    "The result will be a dataFrame including N, end_phase and frequency for all main cycles, traps and positiions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# each ion in each trap its own N determination, for every main cycle! (no grouping here, wouldn't make any sense)\n",
    "\n",
    "columns = [\"mcycle\", \"trap\", \"position\", \"N\", \"end_phase\", \"nu_p\", \"ion\", \"time\", \"max_acc_time\"]\n",
    "nu_p_N = pd.DataFrame(columns = columns)\n",
    "\n",
    "for mc in mcs:\n",
    "    for trap in traps:\n",
    "        for pos in positions:\n",
    "            print(mc, trap, pos)\n",
    "            subset = nu_p_unwrap[(nu_p_unwrap[\"mcycle\"] == mc) & (nu_p_unwrap[\"trap\"] == trap) & (nu_p_unwrap[\"position\"] == pos)]\n",
    "\n",
    "            #subset = subset[subset[\"acc_time\"] != subset[\"acc_time\"].max()]\n",
    "            \n",
    "            # NOTE: if the structure of the config changed, you have to adjust here!\n",
    "            nu_p_guess = meas_config[pos][\"configuration\"][\"traps\"][trap][\"nu_p\"]\n",
    "            mtype = meas_config['type']\n",
    "            if mtype == 'PnA':\n",
    "                nphase = True\n",
    "            else:\n",
    "                nphase = False\n",
    "            ion_str = meas_config[pos][\"configuration\"][\"traps\"][trap][\"ion\"]\n",
    "            #evolution_time = abs(meas_config[\"accumulation_time\"][0][\"time\"] - meas_config[\"accumulation_time\"][1][\"time\"])\n",
    "\n",
    "            #print(\" >>> mc\", mc, \"trap\", trap, \"pos\", pos, \" <<< \")\n",
    "            try:\n",
    "                nu_range = settings.get(\"unwrap_range\", 3)\n",
    "                avg_subset = phase_analysis.prepare_unwrap_phases(subset, val=\"phase\", show=True)\n",
    "                display(avg_subset)\n",
    "                N, end_phase, nu_p, mean_time, max_acc_time, Nquality = phase_analysis.determine_N(avg_subset, nu_p_guess, \n",
    "                                                                        negative=nphase, resolution=None, nu_range=nu_range, show=show_tag)\n",
    "                new_row = pd.Series([mc, trap, pos, N, end_phase, nu_p, ion_str, mean_time, max_acc_time], index=nu_p_N.columns )\n",
    "                print(Nquality)\n",
    "\n",
    "                nu_p_N = nu_p_N.append(new_row, ignore_index=True)\n",
    "            except:\n",
    "                raise\n",
    "\n",
    "# show results and save to csv in results folder\n",
    "display(nu_p_N)\n",
    "nu_p_N.to_csv(results_dir + \"step4_nu_p_N.csv\")\n",
    "nu_p_N.to_csv(results_dir + \"step4_nu_p_N.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW now also for nu_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# each ion in each trap its own N determination, for every main cycle! (no grouping here, wouldn't make any sense)\n",
    "\n",
    "columns = [\"mcycle\", \"trap\", \"position\", \"N\", \"end_phase\", \"nu_m\", \"ion\", \"time\", \"max_acc_time\"]\n",
    "nu_m_N = pd.DataFrame(columns = columns)\n",
    "\n",
    "for mc in mcs:\n",
    "    for trap in traps:\n",
    "        for pos in positions:\n",
    "            print(mc, trap, pos)\n",
    "            subset = nu_m_unwrap[(nu_m_unwrap[\"mcycle\"] == mc) & (nu_m_unwrap[\"trap\"] == trap) & (nu_m_unwrap[\"position\"] == pos)]\n",
    "\n",
    "            #subset = subset[subset[\"acc_time\"] != subset[\"acc_time\"].max()]\n",
    "            \n",
    "            # NOTE: if the structure of the config changed, you have to adjust here!\n",
    "            nu_m_guess = meas_config[pos][\"configuration\"][\"traps\"][trap][\"nu_m\"]\n",
    "            mtype = meas_config['type']\n",
    "            if mtype == 'PnA':\n",
    "                nphase = False\n",
    "            else:\n",
    "                nphase = True\n",
    "            ion_str = meas_config[pos][\"configuration\"][\"traps\"][trap][\"ion\"]\n",
    "            #evolution_time = abs(meas_config[\"accumulation_time\"][0][\"time\"] - meas_config[\"accumulation_time\"][1][\"time\"])\n",
    "\n",
    "            #print(\" >>> mc\", mc, \"trap\", trap, \"pos\", pos, \" <<< \")\n",
    "            try:\n",
    "                nu_range = 10# settings.get(\"unwrap_range\", 5)\n",
    "                avg_subset = phase_analysis.prepare_unwrap_phases(subset, val=\"phase\", show=True)\n",
    "                display(avg_subset)\n",
    "                N, end_phase, nu_m, mean_time, max_acc_time, Nquality = phase_analysis.determine_N(avg_subset, nu_m_guess, \n",
    "                                                                        negative=nphase, resolution=0.0002, nu_range=nu_range, show=show_tag)\n",
    "                new_row = pd.Series([mc, trap, pos, N, end_phase, nu_m, ion_str, mean_time, max_acc_time], index=nu_m_N.columns )\n",
    "                print(Nquality)\n",
    "\n",
    "                nu_m_N = nu_m_N.append(new_row, ignore_index=True)\n",
    "            except:\n",
    "                raise\n",
    "\n",
    "# show results and save to csv in results folder\n",
    "display(nu_m_N)\n",
    "nu_m_N.to_csv(results_dir + \"step4_nu_m_N.csv\")\n",
    "nu_m_N.to_csv(results_dir + \"step4_nu_m_N.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_p_N[\"Nplus\"] = 0\n",
    "backup_nu_p_N = nu_p_N.copy(deep=True)\n",
    "\n",
    "print(\"1/N\", 1/float(nu_p_N.tail(1).N))\n",
    "print(\"N+1/N+1 - N/N\", (float(nu_p_N.tail(2).iloc[0].N)+1)/(float(nu_p_N.tail(1).N)+1) - (float(nu_p_N.tail(2).iloc[0].N))/(float(nu_p_N.tail(1).N)))\n",
    "print(\"N+2/N+2 - N/N\", (float(nu_p_N.tail(2).iloc[0].N)+2)/(float(nu_p_N.tail(1).N)+2) - (float(nu_p_N.tail(2).iloc[0].N))/(float(nu_p_N.tail(1).N)))\n",
    "print(\"N+2/N+1 - N/N\", (float(nu_p_N.tail(2).iloc[0].N)+2)/(float(nu_p_N.tail(1).N)+1) - (float(nu_p_N.tail(2).iloc[0].N))/(float(nu_p_N.tail(1).N)))\n",
    "print(\"2/N\", 2/float(nu_p_N.tail(1).N))\n",
    "print(\"3/N\", 3/float(nu_p_N.tail(1).N))\n",
    "print(\"18/N\", 18/float(nu_p_N.tail(1).N))\n",
    "\n",
    "print('tevol dnu(N+1)')\n",
    "print(float(nu_p_N.tail(1).max_acc_time), 1/float(nu_p_N.tail(1).max_acc_time))\n",
    "print(40, 1/40)\n",
    "print(70, 1/70)\n",
    "print(90, 1/90)\n",
    "print(120, 1/120)\n",
    "print(150, 1/150)\n",
    "print(200, 1/200)\n",
    "\n",
    "# check for reasonable N:\n",
    "ionA, ionB = nu_p_N.ion.unique()\n",
    "ion_mass_ratio = ame.get_ion_mass(ionA)[0]/ame.get_ion_mass(ionB)[0]\n",
    "if ion_mass_ratio < 1: ion_mass_ratio = 1/ion_mass_ratio\n",
    "print(\"literature ion mass ratio\", ion_mass_ratio)\n",
    "for gname, grp in nu_p_N.groupby([\"mcycle\", \"trap\"]):\n",
    "    mc, tr = gname\n",
    "    nu_pA = float(grp[grp.ion == ionA].nu_p)\n",
    "    nu_pB = float(grp[grp.ion == ionB].nu_p)\n",
    "    posA = grp[grp.ion == ionA].position.iloc[0]\n",
    "    posB = grp[grp.ion == ionB].position.iloc[0]\n",
    "    axial_grp = axial_data[(axial_data[\"mcycle\"] == mc) & (axial_data[\"trap\"] == tr) ]\n",
    "    nu_zA = float(axial_grp[axial_grp.position == posA].nu_z.mean())\n",
    "    nu_zB = float(axial_grp[axial_grp.position == posB].nu_z.mean())\n",
    "    nu_mA = meas_config[posA][\"configuration\"][\"traps\"][tr][\"nu_m\"]\n",
    "    nu_mB = meas_config[posB][\"configuration\"][\"traps\"][tr][\"nu_m\"]\n",
    "\n",
    "    this_R = np.sqrt(nu_pA**2 + nu_zA**2 + nu_mA**2) / np.sqrt(nu_pB**2 + nu_zB**2 + nu_mB**2)\n",
    "    if this_R < 1: this_R = 1/this_R\n",
    "    print(gname, this_R, this_R - ion_mass_ratio)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# display(backup_nu_p_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_p_N = backup_nu_p_N.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify an N:\n",
    "#nu_p_N.at[0, \"N\"] -= 10 # t2 pos 1\n",
    "#nu_p_N.at[6, \"N\"] -= 1\n",
    "#nu_p_N.at[0, \"N\"] += -1 # t2 pos 2\n",
    "#nu_p_N.at[4, \"N\"] += -1\n",
    "#nu_p_N.at[3, \"N\"] += 1 # t3 pos 1\n",
    "#nu_p_N.at[7, \"N\"] += 1\n",
    "#nu_p_N.at[2, \"N\"] += -1 # t3 pos 2\n",
    "#nu_p_N.at[6, \"N\"] += -1\n",
    "#nu_p_N.at[2, \"N\"] -= 2\n",
    "display(nu_p_N)\n",
    "for grpname, grp in nu_p_N.groupby([\"trap\", \"mcycle\"]):\n",
    "    nup1, nup2 = grp.N.unique()\n",
    "    R = nup1/nup2\n",
    "    if R < 1: R = 1/R\n",
    "    print(grpname, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Frequency determination from phase data\n",
    "\n",
    "The phase data (in this case just for the nu_p phase measurement) is now evaluated to determine the corresponding frequency.\n",
    "\n",
    "The procedure is split into to parts (two cells) and a last cell for data-handling.\n",
    "\n",
    "#### Cell 1: Unwrapping subcycles / substract reference / unwrap main cycle / assign phase error / 3 sigma phase filter (masked)\n",
    "\n",
    "#### Cell 2: Unwrap relative to N determination phase / Calc nu_p via phase data and N determination data (pre or post unwrap data)\n",
    "\n",
    "#### Cell 3: The frequency determination is finished, but data has to be averaged or exanded now for the next anaylsis steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('nu_p phases')\n",
    "display(nu_p_phases)\n",
    "\n",
    "# just check some phase stabilities to have something to look at when your bored.\n",
    "for grpname, grp in nu_p_phases.groupby([\"mcycle\", \"trap\", \"position\"]):\n",
    "    ref_acc = grp[\"acc_time\"].min()\n",
    "    ref_phases = np.unwrap(grp[grp[\"acc_time\"] == ref_acc].phase.to_numpy())\n",
    "    print(grpname, \"ref phase jitter\", ref_phases.std()*180/np.pi)\n",
    "\n",
    "print('nu_m phases')\n",
    "display(nu_m_phases)\n",
    "# just check some phase stabilities to have something to look at when your bored.\n",
    "for grpname, grp in nu_m_phases.groupby([\"mcycle\", \"trap\", \"position\"]):\n",
    "    ref_acc = grp[\"acc_time\"].min()\n",
    "    ref_phases = np.unwrap(grp[grp[\"acc_time\"] == ref_acc].phase.to_numpy())\n",
    "    print(grpname, \"ref phase jitter\", ref_phases.std()*180/np.pi)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# unwrapping, substracting reference phase, unwrapping for full main cycle, phase filter\n",
    "pd.options.display.float_format = '{: .4f}'.format\n",
    "\n",
    "#nu_p_phases2 = nu_p_phases.copy()\n",
    "nu_p_phases['dphase'] = np.nan\n",
    "step5_results = pd.DataFrame()\n",
    "\n",
    "# U N W R A P,   S U B S T R A C T   R E F E R E N C E,   U N W R A P,   P H A S E   F I L T E R\n",
    "for grpname, grp in nu_p_phases.groupby([\"trap\", \"position\", \"mcycle\"]):\n",
    "#for grpname, grp in nu_p_phases.groupby([\"mcycle\", \"trap\", \"position\"]):\n",
    "    print(grpname)\n",
    "    grp_new = filtering.three_sigma(grp, 'amp', around='median', show=False)\n",
    "    grp_new = phase_analysis.unwrap_subsets(grp_new, groupby=[\"cycle\", \"acc_time\"], show=False)\n",
    "    grp_new = phase_analysis.substract_ref_phase(grp_new, mean=settings.get(\"mean_ref_phase\", False), show=False)\n",
    "    \n",
    "    grp_new = phase_analysis.grouped_unwrap(grp_new, \"phase\", reverse=settings[\"post_unwrap\"], median_group=\"cycle\", \n",
    "                                            pi_span=1, fit_N=2, fit_order=1, show=False, first_expected_offset=-np.pi/1.8, correlate=False)\n",
    "    #grp_new = phase_analysis.grouped_unwrap(grp_new, \"phase\", reverse=settings[\"post_unwrap\"], median_group=[\"cycle\"], \n",
    "    #                                        pi_span=1, fit_N=5, fit_order=1, show=False, skip=5)\n",
    "    #grp_new = phase_analysis.grouped_unwrap(grp_new, \"phase\", reverse=not settings[\"post_unwrap\"], median_group=[\"cycle\"], \n",
    "    #                                        pi_span=1, fit_N=2, fit_order=1, show=False)\n",
    "    #grp_new = phase_analysis.grouped_unwrap(grp_new, \"phase\", reverse=settings[\"post_unwrap\"], median_group=[\"cycle\"], \n",
    "    #                                        pi_span=1, fit_N=-3, fit_order=1, show=False)\n",
    "    #grp_new = phase_analysis.grouped_unwrap(grp_new, \"phase\", reverse=not settings[\"post_unwrap\"], median_group=[\"cycle\"], \n",
    "    #                                        pi_span=1, fit_N=-4, fit_order=1, show=False)\n",
    "    #grp_new = phase_analysis.grouped_unwrap(grp_new, \"phase\", reverse=settings[\"post_unwrap\"], median_group=[\"cycle\"], \n",
    "    #                                        pi_span=1, fit_N=8, fit_order=3, show=False)\n",
    "    #grp_new = phase_analysis.grouped_unwrap(grp_new, \"phase\", reverse=not settings[\"post_unwrap\"], median_group=[\"cycle\"], \n",
    "    #                                        pi_span=1, fit_N=-5, fit_order=1, show=False)\n",
    "    #grp_new = phase_analysis.grouped_unwrap(grp_new, \"phase\", reverse=not settings[\"post_unwrap\"], median_group=[\"cycle\"], \n",
    "    #                                        pi_span=1, fit_N=12, fit_order=5, show=False)\n",
    "\n",
    "    # assign error to long phases\n",
    "    grp_new = statistics.assign_stderr_subsets(grp_new, groupby=\"cycle\", val=\"phase\", dval=\"dphase\", global_undrift=True, x=\"time\", degree=5, show=False)\n",
    "    \n",
    "    step5_results = step5_results.append(grp_new, ignore_index=True)\n",
    "\n",
    "step5_results.sort_values(by=\"time\", inplace=True)\n",
    "\n",
    "if show_tag:\n",
    "    for grpname, grp in step5_results.groupby(\"trap\"):\n",
    "        fig, ax = plt.subplots(figsize=(15, 8))\n",
    "        for subname, subgrp in grp.groupby(\"position\"):\n",
    "            t = subgrp.time\n",
    "            phase = subgrp.phase /2/np.pi\n",
    "            dphase = subgrp.dphase /2/np.pi\n",
    "            ax.errorbar(t, phase, yerr=dphase, label=subname)\n",
    "        plt.ylabel(\"phase (2pi)\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for grpname, grp in step5_results.groupby([\"trap\", \"mcycle\"]):\n",
    "    grp.sort_values(by=\"time\", inplace=True)\n",
    "    \n",
    "    if not settings[\"post_unwrap\"]:\n",
    "        start_position_phase = np.median(grp[(grp[\"position\"]==start_position) & (grp[\"cycle\"]==grp.cycle.min())][\"phase\"])\n",
    "        other_position_phase = np.median(grp[(grp[\"position\"]==other_position) & (grp[\"cycle\"]==grp.cycle.min())][\"phase\"])\n",
    "    else:\n",
    "        start_position_phase = np.median(grp[(grp[\"position\"]==start_position) & (grp[\"cycle\"]==grp.cycle.max()-1)][\"phase\"])\n",
    "        other_position_phase = np.median(grp[(grp[\"position\"]==other_position) & (grp[\"cycle\"]==grp.cycle.max()-1)][\"phase\"])\n",
    "    \n",
    "    print(grpname, start_position_phase, other_position_phase)\n",
    "    \n",
    "    grp.loc[grp['position'] == start_position, 'phase'] -= start_position_phase\n",
    "    grp.loc[grp['position'] == other_position, 'phase'] -= other_position_phase\n",
    "    \n",
    "    #plt.plot(grp[\"phase\"].to_numpy())\n",
    "    #plt.show()\n",
    "\n",
    "    #grp = phase_analysis.grouped_unwrap(grp, column='phase', reverse=settings[\"post_unwrap\"], pi_span=1.0, first_expected_offset=-np.pi/2,\n",
    "    #                                     median_group='cycle', fit_N=3, fit_order=1, show=False, correlate=True, skip=2)\n",
    "    #grp = phase_analysis.grouped_unwrap(grp, column='phase', reverse=settings[\"post_unwrap\"], pi_span=1.0,\n",
    "    #                                     median_group='cycle', fit_N=-2, fit_order=1, show=False)\n",
    "    #grp = phase_analysis.grouped_unwrap(grp, column='phase', reverse=settings[\"post_unwrap\"], pi_span=1.0,\n",
    "    #                                     median_group='cycle', fit_N=2, fit_order=1, show=False)\n",
    "    #grp = phase_analysis.grouped_unwrap(grp, column='phase', reverse=not settings[\"post_unwrap\"], pi_span=1.0,\n",
    "    #                                     median_group='cycle', fit_N=-2, fit_order=1, show=False)\n",
    "    \n",
    "    #plt.plot(grp[\"phase\"].to_numpy())\n",
    "    #plt.show()\n",
    "    \n",
    "    grp.loc[grp['position'] == start_position, 'phase'] += start_position_phase\n",
    "    grp.loc[grp['position'] == other_position, 'phase'] += other_position_phase\n",
    "    \n",
    "    #plt.plot(grp[\"phase\"].to_numpy())\n",
    "    #plt.show()\n",
    "    \n",
    "    step5_results.iloc[grp.index] = grp\n",
    "\n",
    "\n",
    "# check night data phase stability between 0 and 4 in the morning (silent mag-field time)\n",
    "for grpname, grp in step5_results.groupby([\"trap\", \"position\"]):\n",
    "    subset = grp.copy(deep=True)\n",
    "    subset = subset.set_index(subset['time'], drop=True)\n",
    "    subsub = subset.between_time('00:00:00', '04:00:30')\n",
    "    print(\"mean night phase jitter trap\", grpname, np.mean(subsub.dphase.unique())*180/np.pi)\n",
    "\n",
    "#for grpname, grp in step5_results.groupby([\"trap\", \"position\"]):\n",
    "#    # unwrap over complete data of this trap/position (this is safer here then in the loop before, since the next main\n",
    "#    # cycles use the already stable fit from before.)\n",
    "#    step5_results[\"phase\"].loc[grp.index] = phase_analysis.drift_unwrap_method(grp[\"phase\"].to_numpy(), pi_span = 1.3, min_fit = 1, times=grp[\"time\"].astype('int64')//1e9)\n",
    "\n",
    "# WARNING THIS DOES ONLY WORK IF YOU DONT HAVE RANDOM PHASES AT THE END OF THE MEASUREMENT!\n",
    "# assign mean phase std (for full trap data) as phase error\n",
    "for grpname, grp in step5_results.groupby([\"trap\", \"position\"]):\n",
    "    mean_std = grp[\"dphase\"].mean()\n",
    "    print(\"mean phase error in trap\", grpname, mean_std*180/np.pi)\n",
    "    idx = grp.index\n",
    "    step5_results[\"dphase\"].loc[idx] = mean_std\n",
    "\n",
    "step5_results['masked'] = False\n",
    "\n",
    "# 3 sigma filter on phase data\n",
    "if settings[\"phase_filter\"]:\n",
    "    for grpname, grp in step5_results.groupby([\"mcycle\", \"trap\", \"position\", \"cycle\"]):\n",
    "        #print(grpname, grp[\"phase\"].std()*180/np.pi)\n",
    "        grp = filtering.three_sigma(grp, 'phase', err=None, undrift_xcolumn='time', manual_std=grp[\"dphase\"].mean(), show=False)\n",
    "        idx = grp.index\n",
    "        step5_results[\"masked\"].loc[idx] = grp[\"masked\"]\n",
    "\n",
    "step5_results.sort_values(by=\"time\", inplace=True)\n",
    "\n",
    "if show_tag:\n",
    "    for grpname, grp in step5_results.groupby(\"trap\"):\n",
    "        fig, ax = plt.subplots(figsize=(15, 8))\n",
    "        for subname, subgrp in grp.groupby(\"position\"):\n",
    "            t = subgrp.time\n",
    "            phase = subgrp.phase /2/np.pi\n",
    "            dphase = subgrp.dphase /2/np.pi\n",
    "            ax.errorbar(t, phase, yerr=dphase, label=subname)\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "maskedphases = step5_results[ step5_results[\"masked\"] == True ]\n",
    "display(maskedphases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# unwrapping, substracting reference phase, unwrapping for full main cycle, phase filter\n",
    "\n",
    "#nu_p_phases2 = nu_p_phases.copy()\n",
    "nu_m_phases['dphase'] = np.nan\n",
    "step5_results_m = pd.DataFrame()\n",
    "\n",
    "# U N W R A P,   S U B S T R A C T   R E F E R E N C E,   U N W R A P,   P H A S E   F I L T E R\n",
    "for grpname, grp in nu_m_phases.groupby([\"trap\", \"position\", \"mcycle\"]):\n",
    "    print(grpname)\n",
    "    grp_new = filtering.three_sigma(grp, 'amp', around='median', show=False)\n",
    "    grp_new = phase_analysis.unwrap_subsets(grp_new, groupby=[\"cycle\", \"acc_time\"], show=False)\n",
    "    grp_new = phase_analysis.substract_ref_phase(grp_new, mean=settings.get(\"mean_ref_phase\", False), show=False)\n",
    "    \n",
    "    grp_new = phase_analysis.grouped_unwrap(grp_new, \"phase\", reverse=settings[\"post_unwrap\"], median_group=\"cycle\", \n",
    "                                            pi_span=1, fit_N=2, fit_order=1, show=False, first_expected_offset=0, correlate=False)\n",
    "    grp_new = phase_analysis.grouped_unwrap(grp_new, \"phase\", reverse=not settings[\"post_unwrap\"], median_group=[\"cycle\"], \n",
    "                                            pi_span=1, fit_N=5, fit_order=1, show=False, skip=5)\n",
    "    grp_new = phase_analysis.grouped_unwrap(grp_new, \"phase\", reverse=not settings[\"post_unwrap\"], median_group=[\"cycle\"], \n",
    "                                            pi_span=1, fit_N=2, fit_order=1, show=False)\n",
    "    grp_new = phase_analysis.grouped_unwrap(grp_new, \"phase\", reverse=not settings[\"post_unwrap\"], median_group=[\"cycle\"], \n",
    "                                            pi_span=1, fit_N=-10, fit_order=1, show=False)\n",
    "    #grp_new = phase_analysis.grouped_unwrap(grp_new, \"phase\", reverse=not settings[\"post_unwrap\"], median_group=[\"cycle\"], \n",
    "    #                                        pi_span=1, fit_N=-4, fit_order=1, show=False)\n",
    "    #grp_new = phase_analysis.grouped_unwrap(grp_new, \"phase\", reverse=settings[\"post_unwrap\"], median_group=[\"cycle\"], \n",
    "    #                                        pi_span=1, fit_N=8, fit_order=3, show=False)\n",
    "    #grp_new = phase_analysis.grouped_unwrap(grp_new, \"phase\", reverse=not settings[\"post_unwrap\"], median_group=[\"cycle\"], \n",
    "    #                                        pi_span=1, fit_N=-5, fit_order=1, show=False)\n",
    "    #grp_new = phase_analysis.grouped_unwrap(grp_new, \"phase\", reverse=not settings[\"post_unwrap\"], median_group=[\"cycle\"], \n",
    "    #                                        pi_span=1, fit_N=12, fit_order=5, show=False)\n",
    "\n",
    "    # assign error to long phases\n",
    "    grp_new = statistics.assign_stderr_subsets(grp_new, groupby=\"cycle\", val=\"phase\", dval=\"dphase\", global_undrift=True, x=\"time\", degree=5, show=False)\n",
    "    \n",
    "    step5_results_m = step5_results_m.append(grp_new, ignore_index=True)\n",
    "\n",
    "step5_results_m.sort_values(by=\"time\", inplace=True)\n",
    "\n",
    "if show_tag:\n",
    "    for grpname, grp in step5_results_m.groupby(\"trap\"):\n",
    "        fig, ax = plt.subplots(figsize=(15, 8))\n",
    "        for subname, subgrp in grp.groupby(\"position\"):\n",
    "            t = subgrp.time\n",
    "            phase = subgrp.phase /2/np.pi\n",
    "            dphase = subgrp.dphase /2/np.pi\n",
    "            ax.errorbar(t, phase, yerr=dphase, label=subname)\n",
    "        plt.ylabel(\"phase (2pi)\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for grpname, grp in step5_results_m.groupby([\"trap\", \"mcycle\"]):\n",
    "    grp.sort_values(by=\"time\", inplace=True)\n",
    "    \n",
    "    if not settings[\"post_unwrap\"]:\n",
    "        start_position_phase = np.median(grp[(grp[\"position\"]==start_position) & (grp[\"cycle\"]==grp.cycle.min())][\"phase\"])\n",
    "        other_position_phase = np.median(grp[(grp[\"position\"]==other_position) & (grp[\"cycle\"]==grp.cycle.min())][\"phase\"])\n",
    "    else:\n",
    "        start_position_phase = np.median(grp[(grp[\"position\"]==start_position) & (grp[\"cycle\"]==grp.cycle.max()-1)][\"phase\"])\n",
    "        other_position_phase = np.median(grp[(grp[\"position\"]==other_position) & (grp[\"cycle\"]==grp.cycle.max()-1)][\"phase\"])\n",
    "    \n",
    "    print(grpname, start_position_phase, other_position_phase)\n",
    "    \n",
    "    grp.loc[grp['position'] == start_position, 'phase'] -= start_position_phase\n",
    "    grp.loc[grp['position'] == other_position, 'phase'] -= other_position_phase\n",
    "    \n",
    "    #plt.plot(grp[\"phase\"].to_numpy())\n",
    "    #plt.show()\n",
    "\n",
    "    grp = phase_analysis.grouped_unwrap(grp, column='phase', reverse=settings[\"post_unwrap\"], pi_span=1.0, first_expected_offset=0,\n",
    "                                         median_group='cycle', fit_N=4, fit_order=1, show=False, correlate=True, skip=0)\n",
    "    #grp = phase_analysis.grouped_unwrap(grp, column='phase', reverse=settings[\"post_unwrap\"], pi_span=1.0,\n",
    "    #                                     median_group='cycle', fit_N=-2, fit_order=1, show=False)\n",
    "    #grp = phase_analysis.grouped_unwrap(grp, column='phase', reverse=settings[\"post_unwrap\"], pi_span=1.0,\n",
    "    #                                     median_group='cycle', fit_N=2, fit_order=1, show=False)\n",
    "    #grp = phase_analysis.grouped_unwrap(grp, column='phase', reverse=not settings[\"post_unwrap\"], pi_span=1.0,\n",
    "    #                                     median_group='cycle', fit_N=-2, fit_order=1, show=False)\n",
    "    \n",
    "    #plt.plot(grp[\"phase\"].to_numpy())\n",
    "    #plt.show()\n",
    "    \n",
    "    grp.loc[grp['position'] == start_position, 'phase'] += start_position_phase\n",
    "    grp.loc[grp['position'] == other_position, 'phase'] += other_position_phase\n",
    "    \n",
    "    #plt.plot(grp[\"phase\"].to_numpy())\n",
    "    #plt.show()\n",
    "    \n",
    "    step5_results_m.iloc[grp.index] = grp\n",
    "\n",
    "\n",
    "# check night data phase stability between 0 and 4 in the morning (silent mag-field time)\n",
    "for grpname, grp in step5_results_m.groupby([\"trap\", \"position\"]):\n",
    "    subset = grp.copy(deep=True)\n",
    "    subset = subset.set_index(subset['time'], drop=True)\n",
    "    subsub = subset.between_time('00:00:00', '04:00:30')\n",
    "    print(\"mean night phase jitter trap\", grpname, np.mean(subsub.dphase.unique())*180/np.pi)\n",
    "\n",
    "#for grpname, grp in step5_results.groupby([\"trap\", \"position\"]):\n",
    "#    # unwrap over complete data of this trap/position (this is safer here then in the loop before, since the next main\n",
    "#    # cycles use the already stable fit from before.)\n",
    "#    step5_results[\"phase\"].loc[grp.index] = phase_analysis.drift_unwrap_method(grp[\"phase\"].to_numpy(), pi_span = 1.3, min_fit = 1, times=grp[\"time\"].astype('int64')//1e9)\n",
    "\n",
    "# WARNING THIS DOES ONLY WORK IF YOU DONT HAVE RANDOM PHASES AT THE END OF THE MEASUREMENT!\n",
    "# assign mean phase std (for full trap data) as phase error\n",
    "for grpname, grp in step5_results_m.groupby([\"trap\", \"position\"]):\n",
    "    mean_std = grp[\"dphase\"].mean()\n",
    "    print(\"mean phase error in trap\", grpname, mean_std*180/np.pi)\n",
    "    idx = grp.index\n",
    "    step5_results_m[\"dphase\"].loc[idx] = mean_std\n",
    "\n",
    "step5_results_m['masked'] = False\n",
    "\n",
    "# 3 sigma filter on phase data\n",
    "if settings[\"phase_filter\"]:\n",
    "    for grpname, grp in step5_results_m.groupby([\"mcycle\", \"trap\", \"position\", \"cycle\"]):\n",
    "        #print(grpname, grp[\"phase\"].std()*180/np.pi)\n",
    "        grp = filtering.three_sigma(grp, 'phase', err=None, undrift_xcolumn='time', manual_std=grp[\"dphase\"].mean(), show=False)\n",
    "        idx = grp.index\n",
    "        step5_results_m[\"masked\"].loc[idx] = grp[\"masked\"]\n",
    "\n",
    "step5_results_m.sort_values(by=\"time\", inplace=True)\n",
    "\n",
    "if show_tag:\n",
    "    for grpname, grp in step5_results_m.groupby(\"trap\"):\n",
    "        fig, ax = plt.subplots(figsize=(15, 8))\n",
    "        for subname, subgrp in grp.groupby(\"position\"):\n",
    "            t = subgrp.time\n",
    "            phase = subgrp.phase /2/np.pi\n",
    "            dphase = subgrp.dphase /2/np.pi\n",
    "            ax.errorbar(t, phase, yerr=dphase, label=subname)\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "maskedphases = step5_results_m[ step5_results_m[\"masked\"] == True ]\n",
    "display(maskedphases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# quick correlation check NU P\n",
    "phases = {}\n",
    "maxl = 1e6\n",
    "for mc, mgrp in step5_results.groupby([\"mcycle\"]):\n",
    "    for grpname, grp in mgrp.groupby([\"trap\", \"position\"]):\n",
    "        ph = grp.phase.to_numpy()\n",
    "        ph = ph-ph.min()\n",
    "        t = (grp.time.astype('int64')//1e9).to_numpy()\n",
    "        x = t - t.min()\n",
    "        #print(x, ph)\n",
    "        coef = np.polyfit(x,ph,7)\n",
    "        ph -= np.poly1d(coef)(x)\n",
    "        label = str(grpname[0])+str(grpname[1].split(\"_\")[1])\n",
    "        plt.plot(x, ph + 10*int(label[1]), \"o\", label=label)\n",
    "        phases[label] = ph\n",
    "        if len(ph) < maxl:\n",
    "            maxl = len(ph)\n",
    "    plt.title(\"correlation check mc\" +str(mc) )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    try:\n",
    "        print(\"mcycle\", mc)\n",
    "        print(\"correlation position 1\", scipy.stats.spearmanr(phases[\"21\"][:maxl], phases[\"31\"][:maxl]))\n",
    "        print(\"correlation position 2\", scipy.stats.spearmanr(phases[\"22\"][:maxl], phases[\"32\"][:maxl]))\n",
    "        print(\"correlation trap 2\", scipy.stats.spearmanr(phases[\"21\"][:maxl], phases[\"22\"][:maxl]))\n",
    "        print(\"correlation trap 3\", scipy.stats.spearmanr(phases[\"31\"][:maxl], phases[\"32\"][:maxl]))\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# quick correlation check NU M\n",
    "phases = {}\n",
    "maxl = 1e6\n",
    "for mc, mgrp in step5_results_m.groupby([\"mcycle\"]):\n",
    "    for grpname, grp in mgrp.groupby([\"trap\", \"position\"]):\n",
    "        ph = grp.phase.to_numpy()\n",
    "        ph = ph-ph.min()\n",
    "        t = (grp.time.astype('int64')//1e9).to_numpy()\n",
    "        x = t - t.min()\n",
    "        #print(x, ph)\n",
    "        coef = np.polyfit(x,ph,7)\n",
    "        ph -= np.poly1d(coef)(x)\n",
    "        label = str(grpname[0])+str(grpname[1].split(\"_\")[1])\n",
    "        plt.plot(x, ph + 10*int(label[1]), \"o\", label=label)\n",
    "        phases[label] = ph\n",
    "        if len(ph) < maxl:\n",
    "            maxl = len(ph)\n",
    "    plt.title(\"correlation check mc\" +str(mc) )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    try:\n",
    "        print(\"mcycle\", mc)\n",
    "        print(\"correlation position 1\", scipy.stats.spearmanr(phases[\"21\"][:maxl], phases[\"31\"][:maxl]))\n",
    "        print(\"correlation position 2\", scipy.stats.spearmanr(phases[\"22\"][:maxl], phases[\"32\"][:maxl]))\n",
    "        print(\"correlation trap 2\", scipy.stats.spearmanr(phases[\"21\"][:maxl], phases[\"22\"][:maxl]))\n",
    "        print(\"correlation trap 3\", scipy.stats.spearmanr(phases[\"31\"][:maxl], phases[\"32\"][:maxl]))\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qgrid_widget = qgrid.show_grid(nu_p_N)\n",
    "qgrid_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_p_N = qgrid_widget.get_changed_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calc frequency from phase using N_determination from unwrap data\n",
    "step5_results['nu_p'] = np.nan\n",
    "step5_results['dnu_p'] = np.nan\n",
    "\n",
    "#settings[\"post_unwrap\"] = not settings[\"post_unwrap\"]\n",
    "step5_results.sort_values(by=\"time\", inplace=True)\n",
    "\n",
    "wait = True\n",
    "phaseminmax = [0, 0]\n",
    "for grpname, grp in step5_results.groupby([\"mcycle\", \"trap\", \"position\"]):\n",
    "    mc, trap, pos = grpname\n",
    "\n",
    "    # get the results from the unwrap measurement (pre and post)\n",
    "    N_data = nu_p_N[(nu_p_N['mcycle'] == mc) & (nu_p_N['trap'] == trap) & (nu_p_N['position'] == pos)]\n",
    "    reverse = False\n",
    "\n",
    "    N_data_next = nu_p_N.loc[(nu_p_N['mcycle'] == mc+1) & (nu_p_N['trap'] == trap) & (nu_p_N['position'] == pos)]\n",
    "    if settings[\"post_unwrap\"] and (len(N_data_next) != 0):\n",
    "        N_data = N_data_next\n",
    "        reverse = True\n",
    "        print(\" >>> WARNING: using Post-Unwrap! <<< \")\n",
    "    else:\n",
    "        print(\" >>> NORMAL: using Pre-Unwrap! <<< \")\n",
    "        \n",
    "    #display(N_data)\n",
    "    last_phase = float(N_data[\"end_phase\"])\n",
    "    last_time = int(N_data[\"time\"].astype('int64')//1e9)\n",
    "    N = float(N_data[\"N\"])\n",
    "    N += float(N_data[\"Nplus\"])\n",
    "    acc_meas = grp[\"acc_time\"].mean()\n",
    "    acc_Ndet = float(N_data[\"max_acc_time\"])\n",
    "    acc_diff = np.around(acc_Ndet - acc_meas, 3)\n",
    "    print(acc_diff)\n",
    "    if acc_diff != 0:\n",
    "        dN = float(N_data[\"nu_p\"])*acc_diff\n",
    "        dphase = dN - np.around(dN, 0)\n",
    "        print(\"dN\", dN, np.around(dN, 0), last_phase, dphase)\n",
    "        N -= np.around(dN, 0)\n",
    "        last_phase -= dphase\n",
    "\n",
    "    # now we have to unwrap all the phases in this main cycle using the last phase of the\n",
    "    # N determination as a starting phase. This way we assure that the N from the N_determination\n",
    "    # fits the phases in the measurement.\n",
    "    #\"\"\"\n",
    "    # fit phases:\n",
    "    N_phases = 30\n",
    "    phases = grp[\"phase\"].to_numpy()\n",
    "    times = grp[\"epoch\"].to_numpy()\n",
    "    if settings[\"post_unwrap\"] and (len(N_data_next) != 0):\n",
    "        print(\"post unwrap\")\n",
    "        fit_phases = phases[-N_phases:]\n",
    "        fit_times = times[-N_phases:]\n",
    "    else:\n",
    "        print(\"pre unwrap\")\n",
    "        fit_phases = grp[\"phase\"].to_numpy()[:N_phases]\n",
    "        fit_times = grp[\"epoch\"].to_numpy()[:N_phases]\n",
    "    coef = np.polyfit(fit_times, fit_phases, 1)\n",
    "    poly1d_fn = np.poly1d(coef)\n",
    "    expected_value = poly1d_fn(last_time)\n",
    "    delta = expected_value - last_phase\n",
    "    counter = 0\n",
    "    print(\"init delta\", delta)\n",
    "    while abs(delta) > np.pi:\n",
    "        sign = delta/abs(delta)\n",
    "        phases -= 2*np.pi * sign\n",
    "        delta -= 2*np.pi * sign\n",
    "        print(\"new delta\", delta)\n",
    "        counter += 1 * sign\n",
    "        \n",
    "    grp[\"phase\"] = phases\n",
    "    if True:\n",
    "        plt.plot(times, phases/np.pi/2, \".\", label=\"phases\")\n",
    "        plt.plot([last_time], [last_phase/np.pi/2], \"o\", label=\"N phase\")\n",
    "        plt.plot([last_time], [(poly1d_fn(last_time)-counter*2*np.pi)/np.pi/2], \"v\", label=\"expected N phase\")\n",
    "        plt.plot(times, (poly1d_fn(times)-counter*2*np.pi)/np.pi/2)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    # get expected aat current phase time\n",
    "    #\"\"\"\n",
    "    \n",
    "    #grp = phase_analysis.grouped_unwrap(grp, column=\"phase\", start_phase_time = (last_phase, N_data[\"time\"]), \n",
    "    #                                    reverse = reverse, fit_N=-10, fit_order=1, timesort=\"time\", pi_span=1.0, show=False)\n",
    "\n",
    "    \"\"\"\n",
    "    grp = phase_analysis.unwrap_dset(grp, column=[\"phase\"], start_phase = last_phase, reverse = reverse,\n",
    "                                     #drift_unwrap=False, drift_pi_span=1.2, show=False)\n",
    "                                     #drift_unwrap=\"timex\", drift_pi_span=1.0, timesort = True, start_phase_time = N_data[\"time\"], show=False)\n",
    "                                     drift_unwrap=\"timex\", drift_pi_span=1.1, timesort = True, start_phase_time = N_data[\"time\"], slope=-1.0e-3, show=False)\n",
    "                                     #drift_unwrap=\"justfixit\", drift_pi_span=1.1, timesort = True, start_phase_time = None, show=False)\n",
    "                                     #drift_unwrap=False, drift_pi_span=1.3, timesort = True, start_phase_time = None, show=False)\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculating frequency and error... \n",
    "    grp[\"nu_p\"] = phase_analysis.calc_nu(N, grp[\"acc_time\"], grp[\"phase\"])\n",
    "    grp[\"dnu_p\"] = phase_analysis.calc_dnu(grp[\"acc_time\"], grp[\"dphase\"])\n",
    "    print(grpname, grp[\"dphase\"].mean(), grp[\"dnu_p\"].mean(), grp[\"nu_p\"].mean(), N, grp[\"acc_time\"])\n",
    "    \n",
    "    step5_results.loc[grp.index] = grp\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    if True:        \n",
    "        #plt.errorbar(grp.time, grp.phase/2/np.pi, grp.dphase/2/np.pi, marker='.', label=grpname)\n",
    "        plt.errorbar(grp.epoch, grp.phase/2/np.pi, grp.dphase/2/np.pi, marker='.', label=grpname)\n",
    "        #plt.plot(N_data[\"time\"], last_phase/2/np.pi, marker='o', label=grpname)\n",
    "        plt.plot(last_time, last_phase/2/np.pi, marker='o', label=grpname)\n",
    "        #plt.show()\n",
    "        \n",
    "        #plt.plot(N_data[\"time\"], (N+last_phase/2/np.pi)/float(N_data[\"max_acc_time\"]), marker='o')\n",
    "        if wait:\n",
    "            wait = False\n",
    "            #pass\n",
    "        else:\n",
    "            ax = plt.gca()\n",
    "            ax.yaxis.set_major_formatter(tck.FormatStrFormatter('%g $\\pi$'))\n",
    "            ax.yaxis.set_major_locator(tck.MultipleLocator(base=1.0))\n",
    "\n",
    "            #plt.style.use(\"ggplot\")\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            wait = True\n",
    "    \"\"\"\n",
    "\"\"\"\n",
    "for grpname, grp in step5_results.groupby([\"trap\", \"position\"]):\n",
    "    trap, pos = grpname\n",
    "    \n",
    "    N_data = nu_p_N[(nu_p_N['mcycle'] == 1) & (nu_p_N['trap'] == trap) & (nu_p_N['position'] == pos)]\n",
    "    acc_max = float(N_data[\"max_acc_time\"])\n",
    "    delta_N_freq_shift = 1/acc_max/2 # this is basically 2 pi /2, so the pi step\n",
    "    pi_factor = delta_N_freq_shift/(np.pi*0.5)\n",
    "    print(delta_N_freq_shift, pi_factor)\n",
    "    \n",
    "    grp = phase_analysis.grouped_unwrap(grp, column=\"nu_p\", fit_N=24, fit_order=3, reverse=False, timesort=\"time\", pi_span=pi_factor, show=True)\n",
    "\n",
    "    step5_results.loc[grp.index] = grp\n",
    "\"\"\" \n",
    "#step5_results.sort_values(by='time', inplace=True)\n",
    "#display(step5_results)\n",
    "step5_results.to_csv(results_dir + \"step5_nu_p_values.csv\")\n",
    "step5_results.to_csv(results_dir + \"step5_nu_p_values.txt\", sep=\"\\t\")\n",
    "print(\"> masked data:\")\n",
    "maskedphases = step5_results[ step5_results[\"masked\"] == True ]\n",
    "display(maskedphases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(step5_results, x=\"time\", y=\"nu_p\", error_y=\"dnu_p\", facet_col=\"trap\", facet_row=\"position\", color=\"masked\", hover_data=['mcycle', 'cycle', 'position'])\n",
    "#fig = px.scatter(step5_results, x=\"time\", y=\"phase\", error_y=\"dphase\", facet_col=\"trap\", facet_row=\"position\", color=\"masked\", hover_data=['mcycle', 'cycle', 'position'])\n",
    "fig.update_yaxes(matches=None, showticklabels=True)\n",
    "fig.show()\n",
    "display(nu_p_N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calc frequency from phase using N_determination from unwrap data NU M\n",
    "step5_results_m['nu_p'] = np.nan\n",
    "step5_results_m['dnu_p'] = np.nan\n",
    "\n",
    "#settings[\"post_unwrap\"] = not settings[\"post_unwrap\"]\n",
    "step5_results_m.sort_values(by=\"time\", inplace=True)\n",
    "\n",
    "wait = True\n",
    "phaseminmax = [0, 0]\n",
    "for grpname, grp in step5_results_m.groupby([\"mcycle\", \"trap\", \"position\"]):\n",
    "    mc, trap, pos = grpname\n",
    "\n",
    "    # get the results from the unwrap measurement (pre and post)\n",
    "    N_data = nu_m_N[(nu_m_N['mcycle'] == mc) & (nu_m_N['trap'] == trap) & (nu_m_N['position'] == pos)]\n",
    "    reverse = False\n",
    "\n",
    "    N_data_next = nu_m_N.loc[(nu_m_N['mcycle'] == mc+1) & (nu_m_N['trap'] == trap) & (nu_m_N['position'] == pos)]\n",
    "    if settings[\"post_unwrap\"] and (len(N_data_next) != 0):\n",
    "        N_data = N_data_next\n",
    "        reverse = True\n",
    "        print(\" >>> WARNING: using Post-Unwrap! <<< \")\n",
    "    else:\n",
    "        print(\" >>> NORMAL: using Pre-Unwrap! <<< \")\n",
    "        \n",
    "    #display(N_data)\n",
    "    last_phase = float(N_data[\"end_phase\"])\n",
    "    last_time = int(N_data[\"time\"].astype('int64')//1e9)\n",
    "    N = float(N_data[\"N\"])\n",
    "    N += float(N_data[\"Nplus\"])\n",
    "    acc_meas = grp[\"acc_time\"].mean()\n",
    "    acc_Ndet = float(N_data[\"max_acc_time\"])\n",
    "    acc_diff = np.around(acc_Ndet - acc_meas, 3)\n",
    "    print(acc_diff)\n",
    "    if acc_diff != 0:\n",
    "        dN = float(N_data[\"nu_p\"])*acc_diff\n",
    "        dphase = dN - np.around(dN, 0)\n",
    "        print(\"dN\", dN, np.around(dN, 0), last_phase, dphase)\n",
    "        N -= np.around(dN, 0)\n",
    "        last_phase -= dphase\n",
    "\n",
    "    # now we have to unwrap all the phases in this main cycle using the last phase of the\n",
    "    # N determination as a starting phase. This way we assure that the N from the N_determination\n",
    "    # fits the phases in the measurement.\n",
    "    #\"\"\"\n",
    "    # fit phases:\n",
    "    N_phases = 30\n",
    "    phases = grp[\"phase\"].to_numpy()\n",
    "    times = grp[\"epoch\"].to_numpy()\n",
    "    if settings[\"post_unwrap\"] and (len(N_data_next) != 0):\n",
    "        print(\"post unwrap\")\n",
    "        fit_phases = phases[-N_phases:]\n",
    "        fit_times = times[-N_phases:]\n",
    "    else:\n",
    "        print(\"pre unwrap\")\n",
    "        fit_phases = grp[\"phase\"].to_numpy()[:N_phases]\n",
    "        fit_times = grp[\"epoch\"].to_numpy()[:N_phases]\n",
    "    coef = np.polyfit(fit_times, fit_phases, 1)\n",
    "    poly1d_fn = np.poly1d(coef)\n",
    "    expected_value = poly1d_fn(last_time)\n",
    "    delta = expected_value - last_phase\n",
    "    counter = 0\n",
    "    print(\"init delta\", delta)\n",
    "    while abs(delta) > np.pi:\n",
    "        sign = delta/abs(delta)\n",
    "        phases -= 2*np.pi * sign\n",
    "        delta -= 2*np.pi * sign\n",
    "        print(\"new delta\", delta)\n",
    "        counter += 1 * sign\n",
    "        \n",
    "    grp[\"phase\"] = phases\n",
    "    if True:\n",
    "        plt.plot(times, phases/np.pi/2, \".\", label=\"phases\")\n",
    "        plt.plot([last_time], [last_phase/np.pi/2], \"o\", label=\"N phase\")\n",
    "        plt.plot([last_time], [(poly1d_fn(last_time)-counter*2*np.pi)/np.pi/2], \"v\", label=\"expected N phase\")\n",
    "        plt.plot(times, (poly1d_fn(times)-counter*2*np.pi)/np.pi/2)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    # get expected aat current phase time\n",
    "    #\"\"\"\n",
    "    \n",
    "    #grp = phase_analysis.grouped_unwrap(grp, column=\"phase\", start_phase_time = (last_phase, N_data[\"time\"]), \n",
    "    #                                    reverse = reverse, fit_N=-10, fit_order=1, timesort=\"time\", pi_span=1.0, show=False)\n",
    "\n",
    "    \"\"\"\n",
    "    grp = phase_analysis.unwrap_dset(grp, column=[\"phase\"], start_phase = last_phase, reverse = reverse,\n",
    "                                     #drift_unwrap=False, drift_pi_span=1.2, show=False)\n",
    "                                     #drift_unwrap=\"timex\", drift_pi_span=1.0, timesort = True, start_phase_time = N_data[\"time\"], show=False)\n",
    "                                     drift_unwrap=\"timex\", drift_pi_span=1.1, timesort = True, start_phase_time = N_data[\"time\"], slope=-1.0e-3, show=False)\n",
    "                                     #drift_unwrap=\"justfixit\", drift_pi_span=1.1, timesort = True, start_phase_time = None, show=False)\n",
    "                                     #drift_unwrap=False, drift_pi_span=1.3, timesort = True, start_phase_time = None, show=False)\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculating frequency and error... \n",
    "    grp[\"nu_p\"] = phase_analysis.calc_nu(N, grp[\"acc_time\"], grp[\"phase\"])\n",
    "    grp[\"dnu_p\"] = phase_analysis.calc_dnu(grp[\"acc_time\"], grp[\"dphase\"])\n",
    "    print(grpname, grp[\"dphase\"].mean(), grp[\"dnu_p\"].mean(), grp[\"nu_p\"].mean(), N, grp[\"acc_time\"])\n",
    "    \n",
    "    step5_results.loc[grp.index] = grp\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    if True:        \n",
    "        #plt.errorbar(grp.time, grp.phase/2/np.pi, grp.dphase/2/np.pi, marker='.', label=grpname)\n",
    "        plt.errorbar(grp.epoch, grp.phase/2/np.pi, grp.dphase/2/np.pi, marker='.', label=grpname)\n",
    "        #plt.plot(N_data[\"time\"], last_phase/2/np.pi, marker='o', label=grpname)\n",
    "        plt.plot(last_time, last_phase/2/np.pi, marker='o', label=grpname)\n",
    "        #plt.show()\n",
    "        \n",
    "        #plt.plot(N_data[\"time\"], (N+last_phase/2/np.pi)/float(N_data[\"max_acc_time\"]), marker='o')\n",
    "        if wait:\n",
    "            wait = False\n",
    "            #pass\n",
    "        else:\n",
    "            ax = plt.gca()\n",
    "            ax.yaxis.set_major_formatter(tck.FormatStrFormatter('%g $\\pi$'))\n",
    "            ax.yaxis.set_major_locator(tck.MultipleLocator(base=1.0))\n",
    "\n",
    "            #plt.style.use(\"ggplot\")\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            wait = True\n",
    "    \"\"\"\n",
    "\"\"\"\n",
    "for grpname, grp in step5_results.groupby([\"trap\", \"position\"]):\n",
    "    trap, pos = grpname\n",
    "    \n",
    "    N_data = nu_p_N[(nu_p_N['mcycle'] == 1) & (nu_p_N['trap'] == trap) & (nu_p_N['position'] == pos)]\n",
    "    acc_max = float(N_data[\"max_acc_time\"])\n",
    "    delta_N_freq_shift = 1/acc_max/2 # this is basically 2 pi /2, so the pi step\n",
    "    pi_factor = delta_N_freq_shift/(np.pi*0.5)\n",
    "    print(delta_N_freq_shift, pi_factor)\n",
    "    \n",
    "    grp = phase_analysis.grouped_unwrap(grp, column=\"nu_p\", fit_N=24, fit_order=3, reverse=False, timesort=\"time\", pi_span=pi_factor, show=True)\n",
    "\n",
    "    step5_results.loc[grp.index] = grp\n",
    "\"\"\" \n",
    "#step5_results.sort_values(by='time', inplace=True)\n",
    "#display(step5_results)\n",
    "step5_results.to_csv(results_dir + \"step5_nu_p_values.csv\")\n",
    "step5_results.to_csv(results_dir + \"step5_nu_p_values.txt\", sep=\"\\t\")\n",
    "print(\"> masked data:\")\n",
    "maskedphases = step5_results[ step5_results[\"masked\"] == True ]\n",
    "display(maskedphases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision: Averaged or single phases?\n",
    "\n",
    "At this point in the analysis you have to choose if you want to work with averaged data (default) or with single phases. The axial spectra typically have to be averaged to reach enough signal-to-noise-ratio to be able to fit a dip. That results in less nu_z data points than nu_p data points from phases (which we get from every subcycle measurement). If you want to work with averaged data, the phases will be averaged to match the nu_z data. If you want to work with single phases, the axial data will be assigned to all phases with an increased errorbar by np.sqrt(N). This way, averaging afterwords would result in the same axial frequency error again.\n",
    "\n",
    "For the interpolation method, averaging is done seperatly even if you choose single phase. It just doesnt work with single phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# merge with axial data: First just expanding the time-matching axial fits to the phase information!\n",
    "step5_all_freqs = pd.DataFrame()\n",
    "axial_data.sort_values(by=[\"mcycle\", \"trap\", \"cycle\", \"position\"], inplace=True)\n",
    "step5_results.sort_values(by=[\"mcycle\", \"trap\", \"cycle\", \"position\", \"time\"], inplace=True)\n",
    "step5_results_p = step5_results.rename(columns = {'time': 'time_p', 'phase': 'phase_p', 'dphase': 'dphase_p', 'average_idx': 'average_idx_p'})\n",
    "\n",
    "groupby = [\"mcycle\", \"trap\", \"cycle\", \"position\"]\n",
    "\n",
    "lastgrpname = None\n",
    "average_idx = []\n",
    "for grpname, grpphase in step5_results_p.groupby(groupby):\n",
    "    if lastgrpname is None:\n",
    "        lastgrpname = grpname\n",
    "    mc, trap, cyc, pos = grpname\n",
    "    #print('mcycle', mc, 'trap', trap, 'cycle', cyc, 'position', pos)\n",
    "    \n",
    "    grpaxial = axial_data\n",
    "    for key, val in zip(groupby, grpname):\n",
    "        grpaxial = grpaxial[ grpaxial[key] == val ]\n",
    "    \n",
    "    grpaxial_time = grpaxial.set_index('time') # set column 'time' to index\n",
    "    current_axial_index = None\n",
    "\n",
    "    subcycle = 0\n",
    "    for idx, row in grpphase.iterrows():\n",
    "        subcycle += 1\n",
    "        # get the time-wise nearest date for axial frequencies\n",
    "        try:\n",
    "            axial_idx = grpaxial_time.index.get_loc(row.time_p, method='nearest')\n",
    "            axial_row = grpaxial.iloc[axial_idx]\n",
    "        except: continue\n",
    "            \n",
    "        #print('phase cycle', row['cycle'], 'axial cycle', axial_row['cycle'], 'phase time', row['time_p'], 'axial time', axial_row['time'])\n",
    "        axial_row = axial_row.iloc[axial_row.index.isin(['time', 'nu_z', 'dnu_z', 'nu_res', 'dnu_res', 'Q', 'dQ', 'A', 'dA', 'off', 'doff', 'dip_width', 'ddip_width', 'fit_err', 'fit_success', 'average_idx'])]\n",
    "\n",
    "        new_row = row.append(axial_row)\n",
    "        new_row[\"subcycle\"] = subcycle\n",
    "        step5_all_freqs = step5_all_freqs.append(new_row, ignore_index=True)        \n",
    "        \n",
    "        # IMPORTANT! We are expanding the axial data, so we have to assign an expanded error the way that the averaged value gets the same error as before!\n",
    "        if current_axial_index is None:\n",
    "            current_axial_index = axial_idx\n",
    "        \n",
    "        if current_axial_index != axial_idx or lastgrpname != grpname:\n",
    "            new_err = last_dnu_z*np.sqrt(len(average_idx))\n",
    "            step5_all_freqs.loc[average_idx, \"dnu_z\"] = float(new_err)\n",
    "            average_idx = []\n",
    "            lastgrpname = grpname\n",
    "\n",
    "        average_idx.append( step5_all_freqs.index[-1] )\n",
    "        last_dnu_z = axial_row[\"dnu_z\"]\n",
    "\n",
    "step5_all_freqs = data_conversion.fix_column_dtypes(step5_all_freqs)\n",
    "\n",
    "if show_tag:\n",
    "    fig = px.scatter(step5_all_freqs, x=\"time_p\", y=\"nu_p\", error_y=\"dnu_p\", facet_col=\"trap\", facet_row=\"position\", color=\"masked\", hover_data=['mcycle', 'cycle', 'position'])\n",
    "    fig.update_yaxes(matches=None, showticklabels=True)\n",
    "    fig.show()\n",
    "\n",
    "step5_all_freqs.to_csv(results_dir + \"step5_all_freqs_values.csv\")\n",
    "step5_all_freqs.to_csv(results_dir + \"step5_all_freqs_values.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# average:\n",
    "if settings.get(\"average\", False):\n",
    "    new_df = pd.DataFrame()\n",
    "    for grpname, grpphase in step5_all_freqs.groupby([\"mcycle\", \"trap\", \"cycle\", \"position\"]):\n",
    "        avg = statistics.average_subsets(grpphase, groupby=[\"average_idx\"], errortype=\"weighted\",\n",
    "                                              columns=[\"phase_p\", \"nu_p\", \"nu_z\", \"time_p\"], \n",
    "                                              dcolumns=[\"dphase_p\", \"dnu_p\", \"dnu_z\", None],\n",
    "                                              masked=True)\n",
    "        new_df = new_df.append(avg, ignore_index=True)\n",
    "        cyc = grpname[2]\n",
    "        if cyc == 1:\n",
    "            pos = grpname[3]\n",
    "            trap = grpname[1]\n",
    "            nu_p = meas_config[pos][\"configuration\"][\"traps\"][trap][\"nu_p\"]\n",
    "            nu_p_here = avg.iloc[0].nu_p\n",
    "            print(trap, pos, nu_p, nu_p_here, nu_p_here - nu_p)\n",
    "    \n",
    "    step5_all_freqs = new_df\n",
    "    for grpname, grp in step5_all_freqs.groupby([\"trap\", \"position\"]):\n",
    "        print('trap, position', grpname, np.std(grp.phase_p)*180/np.pi)\n",
    "        \n",
    "\n",
    "    \n",
    "    if show_tag:\n",
    "        fig = px.scatter(step5_all_freqs, x=\"time_p\", y=\"nu_p\", error_y=\"dnu_p\", facet_col=\"trap\", facet_row=\"position\", hover_data=['mcycle', 'cycle', 'position'])\n",
    "        fig.update_yaxes(matches=None, showticklabels=True)\n",
    "        fig.show()\n",
    "\n",
    "    step5_all_freqs.to_csv(results_dir + \"step5_all_freqs_values.csv\")\n",
    "    step5_all_freqs.to_csv(results_dir + \"step5_all_freqs_values.txt\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means and compare\n",
    "names = []\n",
    "means = []\n",
    "for grpname, grp in step5_all_freqs.groupby([\"trap\", \"position\"]):\n",
    "    names.append(grpname)\n",
    "    means.append(grp.nu_p.mean())\n",
    "\n",
    "try:\n",
    "    print(\"diffs\")\n",
    "    print(names[0], names[1], means[0]-means[1])\n",
    "    print(names[2], names[3], means[2]-means[3])\n",
    "    print(names[0], names[2], means[0]-means[2])\n",
    "    print(names[1], names[3], means[1]-means[3])\n",
    "    print(\"ratios\")\n",
    "    rguess = means[0]/means[1]\n",
    "    print(names[0], names[1], means[0]/means[1])\n",
    "    print(names[3], names[2], means[3]/means[2])\n",
    "    print(names[0], names[2], means[0]/means[2])\n",
    "    print(names[1], names[3], means[1]/means[3]*rguess)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Calc free cyclotron\n",
    "\n",
    "Use the 3 eigenfrequencies to calculate the free cyclotron frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#step6_results = pd.DataFrame()\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "step6_results = step5_all_freqs.copy()\n",
    "step6_results['nu_c'] = np.nan\n",
    "step6_results['dnu_c'] = np.nan\n",
    "step6_results['nu_m'] = np.nan\n",
    "step6_results['dnu_m'] = np.nan\n",
    "step6_results['nu_c_sb'] = np.nan\n",
    "step6_results['dnu_c_sb'] = np.nan\n",
    "step6_results['ion'] = \"\"\n",
    "\n",
    "last_nu_m = {\n",
    "    2: None, # ion str, nu_m\n",
    "    3: None, # ion str, nu_m\n",
    "    4: None, # ion str, nu_m\n",
    "}\n",
    "\n",
    "for grpname, grp in step6_results.groupby([\"mcycle\", \"trap\", \"position\"]):\n",
    "    mc, trap, pos = grpname\n",
    "    print(grpname)\n",
    "    ion_str = meas_config[pos][\"configuration\"][\"traps\"][trap][\"ion\"]\n",
    "    U0 = meas_config[pos][\"traps\"][trap][\"U0\"]\n",
    "    grp[\"ion\"] = ion_str\n",
    "\n",
    "    # get nu_m from measurement config\n",
    "    nu_m = meas_config[pos][\"configuration\"][\"traps\"][trap][\"nu_m\"]\n",
    "    dnu_m = 0.05\n",
    "    # get nu_z from measurement config\n",
    "    nu_z_conf = meas_config[pos][\"configuration\"][\"traps\"][trap][\"nu_z\"]\n",
    "    \n",
    "    if settings.get(\"nu_m2_from_theory\", False):\n",
    "        B = 7#ufloat(7, 1e-6)\n",
    "        if last_nu_m[trap] is None:\n",
    "            last_nu_m[trap] = [ion_str, U0, nu_m]\n",
    "        else:\n",
    "            theo_last_omm = itp.omegam_ionstr(last_nu_m[trap][0], last_nu_m[trap][1], B=B, nominal=False, nominal_mass=True)\n",
    "            theo_this_omm = itp.omegam_ionstr(ion_str, U0, B=B, nominal=False, nominal_mass=True)\n",
    "            theo_delta_num = (theo_this_omm - theo_last_omm)/2/np.pi\n",
    "            print(\"theoretical num last/this/delta\", theo_last_omm/2/np.pi, theo_this_omm/2/np.pi, theo_delta_num/2/np.pi)\n",
    "            print(\"meas num last/this/delta\", last_nu_m[trap][2], nu_m, nu_m-last_nu_m[trap][2])\n",
    "            old_num = nu_m\n",
    "            nu_m = last_nu_m[trap][2] + theo_delta_num\n",
    "            print(\"meas num this/new/delta\", old_num, nu_m, nu_m-old_num)\n",
    "\n",
    "    #print(\"nu_m\", nu_m, \"dnu_m\", dnu_m)\n",
    "    \n",
    "    grp['nu_m'] = nu_m\n",
    "    grp['dnu_m'] = dnu_m\n",
    "    print('nu_m', nu_m, dnu_m)\n",
    "\n",
    "    \n",
    "    if settings.get(\"fill_nu_z_from_config\", False):\n",
    "        print(\"use config nu_z for NaNs\")\n",
    "        dnu_z = float( settings[\"fill_nu_z_from_config\"] )\n",
    "        grp[\"nu_z\"] = grp[\"nu_z\"].fillna(nu_z_conf)\n",
    "        grp[\"dnu_z\"] = grp[\"dnu_z\"].fillna(dnu_z)\n",
    "        grp=grp.replace({'dnu_z': {0: dnu_z}}) \n",
    "    \n",
    "    if settings.get(\"nu_z_from_config\", False):\n",
    "        print(\"use config nu_z\")\n",
    "        nu_z = nu_z_conf\n",
    "        dnu_z = float( settings[\"fill_nu_z_from_config\"] )\n",
    "    else:\n",
    "        nu_z = grp[\"nu_z\"]\n",
    "        dnu_z = grp[\"dnu_z\"]\n",
    "        \n",
    "    \n",
    "    if pos == \"position_2\":\n",
    "        nu_z += 0#-0.1411\n",
    "    elif pos == \"position_1\":\n",
    "        nu_z += 0#+0.1411\n",
    "    \n",
    "    # calc nu_c\n",
    "    grp[\"nu_c\"], grp[\"dnu_c\"] = frequencies.calc_nu_c_error(\n",
    "        grp[\"nu_p\"], nu_z, nu_m,\n",
    "        grp[\"dnu_p\"], dnu_z, dnu_m\n",
    "    )\n",
    "    #print(grp[\"nu_c\"].mean(), grp[\"nu_p\"].mean(), grp[\"nu_z\"].mean(), nu_m)\n",
    "    \n",
    "    # calc nu_c sideband\n",
    "    grp[\"nu_c_sb\"], grp[\"dnu_c_sb\"] = frequencies.calc_nu_c_sb_error(\n",
    "        grp[\"nu_p\"], nu_m,\n",
    "        grp[\"dnu_p\"], dnu_m\n",
    "    )\n",
    "\n",
    "    # calc error impact of smaller frequencies:\n",
    "    mean_nuc = grp[\"nu_c\"].mean()\n",
    "    mean_dnuc = grp[\"dnu_c\"].mean()\n",
    "    mean_nup = grp[\"nu_p\"].mean()\n",
    "    mean_dnup = grp[\"dnu_p\"].mean()\n",
    "    mean_nuz = grp[\"nu_z\"].mean()\n",
    "    mean_dnuz = grp[\"dnu_z\"].mean()   \n",
    "    mean_num = grp[\"nu_m\"].mean()\n",
    "    mean_dnum = grp[\"dnu_m\"].mean()   \n",
    "    print(\"nup error\", mean_nup/mean_nuc*mean_dnup)\n",
    "    print(\"nuz error\", mean_nuz/mean_nuc*mean_dnuz)\n",
    "    print(\"num error\", mean_num/mean_nuc*mean_dnum)\n",
    "    \n",
    "    # save\n",
    "    step6_results.iloc[grp.index] = grp\n",
    "\n",
    "#display( step6_results[ (step6_results[\"trap\"]==2) & (step6_results[\"position\"]==\"position_2\") ] )\n",
    "if show_tag:\n",
    "    fig = visualization.compare_dset_columns(step6_results, x=[\"time_p\", \"time_p\"], y=[\"nu_c\", \"nu_c_sb\"], yerr=[\"dnu_c\", \"dnu_c_sb\"], facet_col=\"trap\", facet_row=\"position\")\n",
    "\n",
    "#display(step6_results.ion)\n",
    "step6_results.to_csv(results_dir + \"step6_nu_c.csv\")\n",
    "step6_results.to_csv(results_dir + \"step6_nu_c_avg.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Filter data\n",
    "\n",
    "First we apply some automated filters, see the next cell. They should mark the most obvious outlines.\n",
    "Then you should check your frequency results manually: \n",
    "\n",
    "The most convinient option is the filter_plot where you can choose the data you want to see and modify the \"masked\" property of a row in a dataset by clicking the individual point in the plot. \n",
    "\n",
    "Also possible to use is the filter_grid which allows you to filter columns, e.g. only show trap2, only position_1 and/or only after a certain date (TODO: time...), The modification of data is blocked except for the column \"masked\", where you can mark bad data.\n",
    "\n",
    "Masking (setting \"masked\" = True) will not effect the nu_c calculation, but the polynomial fit method and the cancellation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if \"filter_settings\" in data:\n",
    "#    filter_settings = data[\"filter_settings\"]\n",
    "#    display(filter_settings)\n",
    "#    for i in range(0, len(filter_settings)):\n",
    "#        row = filter_settings.iloc[i]\n",
    "#        mcycle = int(row[\"mcycle\"])\n",
    "#        step6_results.at[ (step6_results[\"mcycle\"] == mcycle) & (step6_results[\"cycle\"] < row[\"min_cycle\"]) , \"masked\"] = True\n",
    "#        step6_results.at[ (step6_results[\"mcycle\"] == mcycle) & (step6_results[\"cycle\"] > row[\"max_cycle\"]) , \"masked\"] = True#\n",
    "\n",
    "# MANUAL PREFILTER BY CYCLES:\n",
    "#step6_results.at[(step6_results[\"cycle\"] < 14), \"masked\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A few automatic filtering methods TODO: could someone please check these methods? :D\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "if \"masked\" not in step6_results:\n",
    "    step6_results[\"masked\"] = False\n",
    "\n",
    "step7_results = pd.DataFrame()\n",
    "step6_results = data_conversion.fix_column_dtypes(step6_results)\n",
    "\n",
    "for mc in mcs:\n",
    "    for trap in traps:\n",
    "        for pos in positions:\n",
    "            # get the subset\n",
    "            print(\" >>> FILTERING:\", \"mcycle\", mc, \"trap\", trap, \"pos\", pos)\n",
    "            subset = step6_results[(step6_results[\"mcycle\"] == mc) & (step6_results[\"trap\"] == trap) & (step6_results[\"position\"] == pos)]\n",
    "            \n",
    "            # filter nan values (sometimes happens when an axial spectrum is missing)\n",
    "            subset = filtering.nan_filter(subset, columns=[\"nu_c\"])\n",
    "\n",
    "            # filter by min max boundaries for values, default: min_val: 1, max_val: 1e9:\n",
    "            subset = filtering.minmax_value(subset, val=\"nu_z\")\n",
    "            subset = filtering.minmax_value(subset, val=\"nu_p\")\n",
    "            subset = filtering.minmax_value(subset, val=\"nu_c\")\n",
    "            \n",
    "            # apply autofilter 3-sigma condition: calc mean of values and std\n",
    "            # if value is outside of mean+-3*std, it is masked\n",
    "            subset = filtering.three_sigma(subset, val=\"nu_z\", err=\"dnu_z\", undrift_xcolumn=\"time\", show=False)\n",
    "            subset = filtering.three_sigma(subset, val=\"nu_p\", err=\"dnu_p\", undrift_xcolumn=\"time_p\", show=False)\n",
    "            subset = filtering.three_sigma(subset, val=\"nu_c\", err=\"dnu_c\", undrift_xcolumn=\"time_p\", show=False)\n",
    "            #display(subset)\n",
    "\n",
    "            # apply autofilter sigma-size: if sigma of value is 3 time bigger\n",
    "            # then mean sigma, it is masked\n",
    "            subset = filtering.sigma_size(subset, err=\"dnu_z\")\n",
    "            subset = filtering.sigma_size(subset, err=\"dnu_p\")\n",
    "            subset = filtering.sigma_size(subset, err=\"dnu_c\")\n",
    "            #display(subset)\n",
    "            # save\n",
    "            step7_results = step7_results.append(subset, ignore_index=True)\n",
    "            \n",
    "#display(step7_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step7_results = data_conversion.fix_column_dtypes(step7_results)\n",
    "\n",
    "if show_tag:\n",
    "    fig = px.scatter(step7_results, x=\"time_p\", y=\"nu_c\", error_y=\"dnu_c\", facet_col=\"trap\", facet_row=\"position\", color=\"masked\", hover_data=['mcycle', 'cycle', 'position'])\n",
    "    #fig = px.scatter(step7_results, x=\"time_p\", y=\"nu_c\", error_y=\"dnu_c\", facet_col=\"trap\", color=\"position\", hover_data=['mcycle', 'cycle', 'position'])\n",
    "    fig.update_yaxes(matches=None, showticklabels=True)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step7_results_p1 = step7_results[step7_results['position'] == 'position_1']\n",
    "step7_results_p2 = step7_results[step7_results['position'] == 'position_2']\n",
    "\n",
    "step7_results_p2['nuc_diff'] = step7_results_p1[\"nu_c\"] - step7_results_p2[\"nu_c\"]\n",
    "step7_results_p2['dnuc_diff'] = np.sqrt(step7_results_p1[\"dnu_c\"]**2 + step7_results_p2[\"dnu_c\"]**2)\n",
    "if show_tag:\n",
    "    fig = px.scatter(step7_results_p2, x=\"time_p\", y=\"nuc_diff\", error_y=\"dnuc_diff\", facet_col=\"trap\", color=\"masked\", hover_data=['mcycle', 'cycle', 'position'])\n",
    "    #fig = px.scatter(step7_results, x=\"time_p\", y=\"nu_c\", error_y=\"dnu_c\", facet_col=\"trap\", color=\"position\", hover_data=['mcycle', 'cycle', 'position'])\n",
    "    fig.update_yaxes(matches=None, showticklabels=True)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive plots for masking: choose the mcycle, trap, position and frequency you want to check and \n",
    "# just double click on a point to mask/unmask it. Changes are changed automatically.\n",
    "step7_results['epoch_p'] = step7_results['time_p'].astype('int64')//1e9\n",
    "step7_results = data_conversion.fix_column_dtypes(step7_results)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "widg = \"no plot\"\n",
    "if show_tag:\n",
    "    widg = visualization.filter_plot(step7_results, groupby=[\"mcycle\", \"trap\", \"position\"], ydata=[\"nu_c\", \"nu_z\", \"nu_p\", \"nu_c_sb\"], xdata=[\"epoch_p\"])\n",
    "widg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering and checking the data in a table format\n",
    "# be carefull!!!\n",
    "# 1) if you changed \"masked\" values, you have to run the next cell to save them!\n",
    "# 2) if you used the filter option to for example just show trap2 data or only the ones with masked==True, you have to remove these filters\n",
    "#    before you save the changes with execution of the next cell, since you will only save the visible data!\n",
    "\n",
    "qgrid_widget = visualization.filter_grid(step7_results)\n",
    "qgrid_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This has to run once after you changed data in the table view!!!\n",
    "\n",
    "#changed_df = qgrid_widget.get_changed_df()\n",
    "#if len(changed_df) != len(step7_results):\n",
    "#    print( \" >>> WARNING !!! Remove all filters in the table above and run this again.\")\n",
    "#else:\n",
    "#    step7_results = changed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test = step7_results[step7_results['trap']==2]\n",
    "    print(test.nu_c.std(), test.dnu_c.mean(), test.dnu_c.mean()/test.nu_c.mean())\n",
    "    print(test.nu_z.std(), test.dnu_z.mean(), test.dnu_z.mean()/test.nu_z.mean())\n",
    "\n",
    "    nu_z = test.nu_z.to_numpy()\n",
    "    time = test.epoch_p.to_numpy()\n",
    "    time = time - time.min()\n",
    "    time = time/60/60\n",
    "\n",
    "    blubb = visualization.allanvariance(nu_z/np.mean(nu_z), time, plot=True)\n",
    "except:\n",
    "    print(\"trap 2 no data?\")\n",
    "    \n",
    "try:\n",
    "    test = step7_results[step7_results['trap']==3]\n",
    "    print(test.nu_c.std(), test.dnu_c.mean(), test.dnu_c.mean()/test.nu_c.mean())\n",
    "    print(test.nu_z.std(), test.dnu_z.mean(), test.dnu_z.mean()/test.nu_z.mean())\n",
    "\n",
    "    nu_z = test.nu_z.to_numpy()\n",
    "    time = test.epoch_p.to_numpy()\n",
    "    time = time - time.min()\n",
    "    time = time/60/60\n",
    "\n",
    "    blubb = visualization.allanvariance(nu_z/np.mean(nu_z), time, plot=True)\n",
    "except:\n",
    "    print(\"trap 3 no data?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# NOTE: This has to run once after you changed data in the table view!!!\n",
    "\n",
    "#changed_df = qgrid_widget.get_changed_df()\n",
    "#if len(changed_df) != len(step7_results):\n",
    "#    print( \" >>> WARNING !!! Remove all filters in the table above and run this again.\")\n",
    "#else:\n",
    "#    step7_results = changed_df\n",
    "\n",
    "masked_data = step7_results[ step7_results[\"masked\"]==True ]\n",
    "display(masked_data)\n",
    "step7_results.to_csv(results_dir + \"step7_first_filter.csv\")\n",
    "step7_results.to_csv(results_dir + \"step7_first_filter.txt\", sep=\"\\t\")\n",
    "\n",
    "''' For external  use...\n",
    "min_nu_c = int(step7_results.nu_c.min())\n",
    "min_nu_c = 16680543 + 26\n",
    "min_t = step7_results.epoch_p.min()\n",
    "\n",
    "step7_results[\"nu_c_min\"] = min_nu_c\n",
    "step7_results[\"nu_c_d\"] = step7_results[\"nu_c\"] - min_nu_c\n",
    "step7_results[\"nu_c_d_mhz\"] = step7_results[\"nu_c_d\"] * 1000\n",
    "step7_results[\"dnu_c_mhz\"] = step7_results[\"dnu_c\"] * 1000\n",
    "step7_results[\"seconds\"] = step7_results[\"epoch_p\"] - min_t\n",
    "step7_results[\"minutes\"] = step7_results[\"seconds\"] / 60\n",
    "step7_results[\"hours\"] = step7_results[\"minutes\"] / 60\n",
    "print(min_nu_c)\n",
    "for grpname, grp in step7_results.groupby([\"trap\", \"position\"]):\n",
    "    grp.to_csv(results_dir + \"step7_first_filter_t\"+str(grpname[0])+\"_\"+str(grpname[1])+\".txt\", sep=\"\\t\")\n",
    "    sub = grp[(grp['mcycle']==1) & (grp['cycle']<10)]\n",
    "    sub.to_csv(results_dir + \"step7_first_filter_t\"+str(grpname[0])+\"_\"+str(grpname[1])+\"_sc1-10.txt\", sep=\"\\t\")\n",
    "    display(sub)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOW RATIO DETERMINATIONS!!!\n",
    "\n",
    "#### FIRST: TRAP-WISE ANALYSIS\n",
    "\n",
    "B: Interpolation\n",
    "\n",
    "C: Polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Interpolation\n",
    "Data of two cycles of one position is interpolated to the time of the measurement of the in between position. Then the ratio is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the interpolation we have to use averaged data. \n",
    "step9_data = pd.DataFrame()\n",
    "\n",
    "if not settings[\"average\"]:\n",
    "    for grpname, grpdata in step7_results.groupby([\"mcycle\", \"trap\", \"position\"]):\n",
    "        print(\"mcycle, trap, position\", grpname)\n",
    "\n",
    "        avgdata = pd.DataFrame()\n",
    "\n",
    "        #for subname, subgrpdata in grpdata.groupby(\"cycle\"):\n",
    "\n",
    "        avg = statistics.average_subsets(grpdata, groupby=[\"cycle\"], errortype=\"weighted\",\n",
    "                                              columns=[\"nu_c\", \"phase_p\", \"nu_p\", \"nu_z\", \"time_p\"], \n",
    "                                              dcolumns=[\"dnu_c\", \"dphase_p\", \"dnu_p\", \"dnu_z\", None],\n",
    "                                              masked=True)\n",
    "        avgdata = avgdata.append(avg)\n",
    "\n",
    "        avgdata = data_conversion.fix_column_dtypes(avgdata)\n",
    "\n",
    "        # filter by min max boundaries for values, default: min_val: 1, max_val: 1e9:\n",
    "        avgdata = filtering.minmax_value(avgdata, val=\"nu_z\")\n",
    "        avgdata = filtering.minmax_value(avgdata, val=\"nu_p\")\n",
    "        avgdata = filtering.minmax_value(avgdata, val=\"nu_c\")\n",
    "\n",
    "        # apply autofilter 3-sigma condition: calc mean of values and std\n",
    "        # if value is outside of mean+-3*std, it is masked\n",
    "        avgdata = filtering.three_sigma(avgdata, val=\"nu_z\", err=\"dnu_z\", undrift_xcolumn=\"time\", show=False)\n",
    "        avgdata = filtering.three_sigma(avgdata, val=\"nu_p\", err=\"dnu_p\", undrift_xcolumn=\"time_p\", show=False)\n",
    "        avgdata = filtering.three_sigma(avgdata, val=\"nu_c\", err=\"dnu_c\", undrift_xcolumn=\"time_p\", show=False)\n",
    "        #display(subset)\n",
    "\n",
    "        # apply autofilter sigma-size: if sigma of value is 3 time bigger\n",
    "        # then mean sigma, it is masked\n",
    "        avgdata = filtering.sigma_size(avgdata, err=\"dnu_z\")\n",
    "        avgdata = filtering.sigma_size(avgdata, err=\"dnu_p\")\n",
    "        avgdata = filtering.sigma_size(avgdata, err=\"dnu_c\")\n",
    "\n",
    "        step9_data = step9_data.append(avgdata)\n",
    "\n",
    "    print(\"averaged\")\n",
    "else:\n",
    "    step9_data = step7_results.copy(deep=True)\n",
    "    \n",
    "display(step9_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "non_linear_uncertainty_per_second_std = 0\n",
    "step9_data = step9_data.sort_values(\"time_p\")\n",
    "step9_data.reset_index(drop=True, inplace=True)\n",
    "step9_interpolated_data = pd.DataFrame()\n",
    "step9_interpolate_results = pd.DataFrame()\n",
    "invert = settings.get(\"invert\", True)\n",
    "ions = step9_data.ion.unique()\n",
    "print(ions)\n",
    "\n",
    "for grpname, grpdata in step9_data.groupby([\"mcycle\", \"trap\"]):\n",
    "    print('mcycle, trap', grpname)\n",
    "    if len(grpdata.index - sum(grpdata.index.to_numpy())) < 6:\n",
    "        if len(traps)!=1:\n",
    "            invert = not invert\n",
    "        continue\n",
    "        \n",
    "    # first we try to figure out what non-linearity error we have by interpolating in only one position, so same ion, interpolating odd to even cycles\n",
    "    # and determine the difference of the interpolated value to the actually measured value. The mean abs differene to the measured values corresponds\n",
    "    # to a higher order fluctuation. Without any additional information this fluctuation only holds for the time step from one cycle to the next. Anyway,\n",
    "    # we use it as a fluctuation per second value and then \"interpolate\" this fluctuation to the time step between two measurements (of different ions)\n",
    "    # and use this as a non-linear-uncertainty-per-second value for the actual linear interpolation.\n",
    "    non_linearities_per_second = []\n",
    "    non_linearities_std_per_second = []\n",
    "    mean_time_diffs = []\n",
    "    for pos, posdata in grpdata.groupby(\"position\"):\n",
    "        nonlin, nonlin_std, mean_time_diff  = ratio_analysis.estimate_non_linearity_factor(posdata, y=[\"nu_c\"], yerr=[\"dnu_c\"], x=\"time_p\", identifier=\"cycle\")\n",
    "        non_linearities_per_second.append(nonlin)\n",
    "        non_linearities_std_per_second.append(nonlin_std)\n",
    "        mean_time_diffs.append(mean_time_diff)\n",
    "    \n",
    "    time_diff = np.mean(mean_time_diffs)\n",
    "    non_linear_uncertainty_per_second = np.mean(non_linearities_per_second)\n",
    "    non_linear_uncertainty_per_second_std = np.mean(non_linearities_std_per_second)\n",
    "    print(\"non-linearities\", non_linear_uncertainty_per_second*time_diff/2, non_linear_uncertainty_per_second_std*time_diff/2)        \n",
    "    print(\"relative non-linearities\", non_linear_uncertainty_per_second*time_diff/2/grpdata[\"nu_c\"].mean(), non_linear_uncertainty_per_second_std*time_diff/2/grpdata[\"nu_c\"].mean())        \n",
    "        \n",
    "    reference, interpolate = other_position, start_position # what was measured first in time? Thats the one we want to interpolate\n",
    "    print(reference, interpolate)\n",
    "    \n",
    "    interpolated_data = ratio_analysis.interpolate(grpdata, y=['nu_c'], yerr=['dnu_c'], groupbys=['mcycle', 'trap'], x=\"time_p\",\n",
    "                    identifier=\"position\", id_reference=reference, id_interpolate=interpolate,\n",
    "                    non_linear_uncertainty_per_second=non_linear_uncertainty_per_second_std)\n",
    "    \n",
    "    #display(interpolated_data.head(10))\n",
    "    \n",
    "    step9_interpolated_data = step9_interpolated_data.append(interpolated_data)\n",
    "    \n",
    "    positions = grpdata.position.unique()\n",
    "    grpions = grpdata.ion.unique()\n",
    "    if grpions[0] != ions[0]:\n",
    "        positions = positions[::-1]\n",
    "    \n",
    "    #if not invert:\n",
    "    #    positions = grpdata.position.unique()[::-1] \n",
    "    #else:\n",
    "    #    positions = grpdata.position.unique()\n",
    "    print(positions)\n",
    "    \n",
    "    results = ratio_analysis.calc_ratios(interpolated_data, y=['nu_c'], yerr=['dnu_c'], groupbys=['cycle'], identifier=\"position\", ident_types=positions,\n",
    "            keep_columns=['mcycle', 'trap'], additional_identifiers=['ion', 'time_p'], mean_columns=['time_p', 'time'])\n",
    "\n",
    "    results[\"R\"] = results[\"ratio_nu_c\"]\n",
    "    results[\"dR\"] = results[\"dratio_nu_c\"]\n",
    "    \n",
    "    #if results.R.mean() < 1:\n",
    "    #    results['R'] = 1/results.R\n",
    "    #    results['dR'] = results['R']**2 * results['dR']\n",
    "    #    print(results.keys())\n",
    "        \n",
    "    step9_interpolate_results = step9_interpolate_results.append(results, ignore_index=True)\n",
    "    \n",
    "    #if len(step9_data.trap.unique())!=1:\n",
    "    #    invert = not invert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{: .12f}'.format\n",
    "#step9_interpolate_results = data_conversion.fix_column_dtypes(step9_interpolate_results)\n",
    "#display(step9_interpolated_data[:5])\n",
    "#display(step9_interpolate_results[:5])\n",
    "#display(step8_naive_results[\"ion_numer\"].unique())\n",
    "\n",
    "step9_interpolate_results[\"Rminus\"] = step9_interpolate_results[\"R\"] - 1.0\n",
    "\n",
    "step9_interpolate_results.to_csv(results_dir + \"step9_interpolate_results.csv\")\n",
    "step9_interpolate_results.to_csv(results_dir + \"step9_interpolate_results.txt\", sep=\"\\t\")\n",
    "\n",
    "fig = px.line(step9_interpolate_results, x=\"time_p\", y=\"Rminus\", error_y=\"dratio_nu_c\", facet_row='trap', hover_data=['mcycle', 'cycle', \"ion_numer\", \"ion_denom\"])\n",
    "fig.update_yaxes(matches=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"interpolated \": step9_interpolate_results,\n",
    "}\n",
    "\n",
    "# R_old = 1 + 0.024 393 499 731 #+ 2e-12 # trap2\n",
    "\n",
    "R_old = 0.024393499731\n",
    "print(\"orig\", R_old)\n",
    "print(0.0243934997351 - R_old)\n",
    "print(0.0243934997404 - R_old)\n",
    "\n",
    "visualization.compare(results, groupby=[\"trap\", \"mcycle\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Polynom-fit ratio determinatioin\n",
    "\n",
    "The function ```polynom_fit.fit_sharedpoly_ratio``` does everything you need for shared polynomial fits: looping over group sizes, autogrouping, looping over polynom degrees; you can choose between different fit methods and you get all and the already filtered best settings results back; You can use it on single trap data or on trap ratio data for poly-cancel (later). It does either take single trap data or single trap ratio data. Here the docstring of the function for explanation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(polynom_fit.fit_sharedpoly_ratio.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you still want to try your manual grouping, you can use the following cell to pre-group the data with the ```start_size``` and manually modify the grouping inside the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# you might want to do manual grouping\n",
    "temp_df = pd.DataFrame()\n",
    "for gname, grp in step7_results.groupby(['mcycle', 'trap']):\n",
    "    grp = polynom_fit.auto_group_subset(grp, 5, 5-1)\n",
    "    temp_df = temp_df.append(grp)\n",
    "\n",
    "temp_df.sort_values(['mcycle', 'trap', 'time_p'], inplace=True)\n",
    "temp_df.reset_index(drop=True, inplace=True)\n",
    "#display(step7_results)\n",
    "\n",
    "visualization.manual_grouping_plot(temp_df, x=\"epoch_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step10_polyfit_data = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Rguess = step9_interpolate_results['R'].mean()\n",
    "\n",
    "step10_polyfit = pd.DataFrame()\n",
    "step10_polyfit_best_fits = pd.DataFrame()\n",
    "step10_polyfit_best = pd.DataFrame()\n",
    "degrees = settings.get(\"polydegrees\", 'auto')\n",
    "polygrouping = settings.get(\"polygrouping\", 'auto')\n",
    "mode = settings.get(\"poly_mode\", 'curvefit')\n",
    "crit = settings.get(\"poly_criterion\", 'AICc')\n",
    "invert = settings.get(\"invert\", True)\n",
    "import time\n",
    "start = time.perf_counter()\n",
    "\n",
    "for idx, mc in enumerate(mcs):\n",
    "    for idx2, trap in enumerate(traps):\n",
    "        print(\">>> mcycle:\", mc, \"trap:\", trap)\n",
    "        # get the subset\n",
    "        subset = step10_polyfit_data[ (step10_polyfit_data[\"mcycle\"] == mc) & (step10_polyfit_data[\"trap\"] == trap) ]\n",
    "        if len(subset.index) < 3:\n",
    "            invert = not invert\n",
    "            continue\n",
    "        if len(subset.position.unique()) != 2:\n",
    "            invert = not invert\n",
    "            continue\n",
    "        \n",
    "        # get the testing parameters\n",
    "        if isinstance(polygrouping, str) and polygrouping == 'auto':\n",
    "            max_degree = int(np.ceil(len(subset.cycle.unique())/2))\n",
    "            print(\"max degree\", max_degree)\n",
    "            group_sizes = list(range(3, int(max_degree+1))) # e.g. 24 data points->12 points per position->6/6 is the smallest splitting\n",
    "            group_sizes.append(0) # 0 stands for full main cycle\n",
    "            #group_sizes.append(-1) # -1 stands for the manual grouping\n",
    "        else:\n",
    "            group_sizes = polygrouping\n",
    "        if not group_sizes:\n",
    "            group_sizes = [3]\n",
    "        print(\"group sizes:\", group_sizes)\n",
    "\n",
    "        if isinstance(degrees, str) and degrees == 'auto':\n",
    "            max_degree = min( [ len(subset.cycle.unique())-2, 9 ] ) # maximum degree 15 ( anything else is... too much(?) )\n",
    "            poly_degrees = list( range(1, max_degree) ) # e.g. full data set 24 data points - 2 for ratio and keeping one free\n",
    "        else:\n",
    "            poly_degrees = degrees\n",
    "        if not poly_degrees:\n",
    "            poly_degrees = [1]\n",
    "        print(\"polynom degrees:\", poly_degrees)\n",
    "\n",
    "        # fit with all the settings\n",
    "        subset, results, best_fits, best_results = polynom_fit.fit_sharedpoly_ratio(subset, Rguess, y=\"nu_c\", yerr=\"dnu_c\", data_identifier=\"position\", \n",
    "                                               invert=invert, groupsize=group_sizes, degree=poly_degrees, mode=mode, \n",
    "                                               x=\"time_p\", keep_columns=[\"mcycle\", \"trap\"], bestfit=crit, bestgroupsize=\"chi2red\", show=False)\n",
    "        \n",
    "        step10_polyfit = step10_polyfit.append(results)\n",
    "        step10_polyfit_best_fits = step10_polyfit.append(best_fits)\n",
    "        step10_polyfit_best = step10_polyfit_best.append(best_results)\n",
    "        \n",
    "        print(\">>> EOA mcycle:\", mc, \"trap:\", trap)\n",
    "        \n",
    "        if len(traps)!=1:\n",
    "            invert = not invert\n",
    "                \n",
    "stop = time.perf_counter()\n",
    "print(\"time:\", stop-start)\n",
    "\n",
    "step10_polyfit['Rminus'] = step10_polyfit['R'] - 1\n",
    "step10_polyfit_best_fits['Rminus'] = step10_polyfit_best_fits['R'] - 1\n",
    "step10_polyfit_best['Rminus'] = step10_polyfit_best['R'] - 1\n",
    "display(step10_polyfit_best_fits)\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.options.display.float_format = '{: .12f}'.format\n",
    "display(step10_polyfit.head(10))\n",
    "display(step10_polyfit_best)\n",
    "\n",
    "step10_polyfit.to_csv(results_dir + \"step10_polyfit.csv\")\n",
    "step10_polyfit.to_csv(results_dir + \"step10_polyfit.txt\", sep=\"\\t\")\n",
    "step10_polyfit_best_fits.to_csv(results_dir + \"step10_polyfit_best_fits.csv\")\n",
    "step10_polyfit_best_fits.to_csv(results_dir + \"step10_polyfit_best_fits.txt\", sep=\"\\t\")\n",
    "step10_polyfit_best.to_csv(results_dir + \"step10_polyfit_best.csv\")\n",
    "step10_polyfit_best.to_csv(results_dir + \"step10_polyfit_best.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results\n",
    "polynom_fit.plot_best_results(step7_results, step10_polyfit_best, line_width = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polynom_fit.compare_grouping_fits(step7_results, subset_results, groupby=['trap'], x=\"time_p\", y=\"nu_c\", yerr=\"dnu_c\", ratio='R', line_width=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for grpname, grp in step10_polyfit.groupby([\"mcycle\", \"trap\", \"groupsize\"]):\n",
    "    #fig = px.line(grp, x=\"degree\", y=\"R\", error_y=\"dR\", color=\"group\", hover_data=['group'], title=\"mcycle, trap: \"+str(grpname) )\n",
    "    #fig.show()\n",
    "    \n",
    "Rmean = pd.DataFrame()\n",
    "for grpname, grp in step10_polyfit.groupby([\"mcycle\", \"trap\"]):    \n",
    "    R, inner, outer, chi2red = statistics.complete_mean_and_error(grp.R.to_numpy(), grp.dR.to_numpy())\n",
    "    dR = np.nanmax([inner, outer])\n",
    "    print(grpname, R, inner, outer, chi2red)\n",
    "    Rmean = Rmean.append( pd.Series(data=[grpname[0], grpname[1], R, dR, inner, outer, chi2red], \n",
    "                                    index=[\"mcycle\", \"trap\", \"R\", \"dR\", \"inner\", \"outer\", \"chi2red\"]), \n",
    "                          ignore_index=True )\n",
    "    \n",
    "results = pd.DataFrame()\n",
    "for grpname, grp in step10_polyfit.groupby([\"mcycle\", \"trap\", \"groupsize\", \"degree\"]):\n",
    "    R, inner, outer, chi2red = statistics.complete_mean_and_error(grp.R.to_numpy(), grp.dR.to_numpy())\n",
    "    dR = max([inner, outer])\n",
    "    #print(R-1, inner, outer, chi2red)\n",
    "    Rall = float(Rmean[ (Rmean[\"mcycle\"]==grpname[0]) & (Rmean[\"trap\"]==grpname[1]) ][\"R\"])\n",
    "    results = results.append( pd.Series(data=[grpname[0], grpname[1], grpname[2], grpname[3], R, R-Rall, dR, inner, outer, chi2red], \n",
    "                                        index=[\"mcycle\", \"trap\", \"groupsize\", \"degree\", \"R\", \"R-Rmean\", \"dR\", \"inner\", \"outer\", \"chi2red\"]), \n",
    "                              ignore_index=True )\n",
    "\n",
    "fig = px.line(results, x=\"degree\", y=\"R-Rmean\", error_y=\"dR\", color=\"groupsize\", facet_row=\"mcycle\", facet_col=\"trap\" )\n",
    "fig.update_layout( yaxis = dict( showexponent = 'all', exponentformat = 'e' ) )\n",
    "fig.show()\n",
    "fig = px.line(results, x=\"groupsize\", y=\"R-Rmean\", error_y=\"dR\", color=\"degree\", facet_row=\"mcycle\", facet_col=\"trap\" )\n",
    "fig.update_layout( yaxis = dict( showexponent = 'all', exponentformat = 'e' ) )\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "step10_polyfit_best_fits2 = pd.DataFrame(columns=step10_polyfit_best_fits.columns)\n",
    "for grpname, grp in step10_polyfit.groupby([\"mcycle\", \"trap\", \"groupsize\", \"group\"]):\n",
    "    mincrit = grp[crit].min()\n",
    "    sub = grp[ grp[crit]==mincrit ]\n",
    "    step10_polyfit_best_fits2 = step10_polyfit_best_fits2.append(sub, ignore_index=True)\n",
    "\n",
    "print(len(step10_polyfit_best_fits2))\n",
    "for grpname, grp in step10_polyfit_best_fits2.groupby([\"mcycle\", \"trap\", \"groupsize\"]):\n",
    "    print('mc, trap, size, indexes, total', grpname[0], grpname[1], grpname[2], grp.group.unique(), len(grp), grp.degree.unique())\n",
    "    #display(grp)\n",
    "    R, inner, outer, chi2red = statistics.complete_mean_and_error(grp.R.to_numpy(), grp.dR.to_numpy())\n",
    "    dR = max([inner, outer])\n",
    "    mean_degree = float(grp.degree.mean())\n",
    "    #print(R-1, inner, outer, chi2red)\n",
    "    Rall = float(Rmean[ (Rmean[\"mcycle\"]==grpname[0]) & (Rmean[\"trap\"]==grpname[1]) ][\"R\"])\n",
    "    results = results.append( pd.Series(data=[grpname[0], grpname[1], grpname[2], R, R-1, R-Rall, dR, inner, outer, chi2red, mean_degree], \n",
    "                                        index=[\"mcycle\", \"trap\", \"groupsize\", \"R\", \"Rminus\", \"R-Rmean\", \"dR\", \"inner\", \"outer\", \"chi2red\", \"mean_degree\"]), \n",
    "                              ignore_index=True )\n",
    "    \n",
    "fig = px.line(results, x=\"groupsize\", y=\"R-Rmean\", error_y=\"dR\", facet_row=\"mcycle\", facet_col=\"trap\", color=\"mean_degree\", hover_data=['mcycle', \"R-Rmean\", \"dR\", \"mean_degree\"] )\n",
    "fig.update_layout( yaxis = dict( showexponent = 'all', exponentformat = 'e' ) )\n",
    "fig.show()\n",
    "\n",
    "results.to_csv(results_dir + \"step10_polyfit_grouping_best_for_plot.csv\")\n",
    "results.to_csv(results_dir + \"step10_polyfit_grouping_best_for_plot.txt\", sep=\"\\t\")\n",
    "step10_polyfit_best_fits2.to_csv(results_dir + \"step10_polyfit_best_fits.csv\")\n",
    "step10_polyfit_best_fits2.to_csv(results_dir + \"step10_polyfit_best_fits.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(step10_polyfit_best)\n",
    "chi2s = step10_polyfit_best.chi2red.to_numpy()\n",
    "print(chi2s)\n",
    "print(chi2s.mean(), chi2s.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    #\"ame          \": one_df,\n",
    "    \"interpolated\": step9_interpolate_results,\n",
    "    \"poly        \": step10_polyfit_best,\n",
    "}\n",
    "\n",
    "visualization.compare(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(step10_polyfit_best.columns)\n",
    "display(step10_polyfit_best_fits.AICc)\n",
    "print(step10_polyfit_best.chi2red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SECOND: CANCELLATION ANALYSIS\n",
    "\n",
    "B: Interpolation\n",
    "\n",
    "C: Polynomial\n",
    "\n",
    "##### FIRST: Data preparation (creating trap ratios) --- ONLY WITH 2 TRAP DATA!!! \n",
    "e.g.\n",
    "\n",
    "Rpos1 = nu_c(trap2, t1) / nu_c(trap3, t1) = qmA * B(trap2, t1) / qmB / B(trap3, t1) = qmA/qmB * rohB(t1)\n",
    "\n",
    "Rpos2 = nu_c(trap2, t2) / nu_c(trap3, t2) = qmB * B(trap2, t2) / qmA / B(trap3, t2) = qmB/qmA * rohB(t2)\n",
    "\n",
    "for naive calculation then:\n",
    "\n",
    "Rcancel = sqrt( Rpos1 / Rpos2 ) = sqrt( qmA^2/qmB^2 ) , if rohB(t1) == rohB(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "step11_trap_ratio_data = pd.DataFrame()\n",
    "step7_results.sort_values([\"mcycle\", \"trap\", \"time_p\"], inplace=True)\n",
    "step7_results.reset_index(drop=True, inplace=True)\n",
    "traps = [2,3] # in this position trap 2 is numerator (trap2/trap3)\n",
    "\n",
    "unique_traps = step7_results.trap.unique()\n",
    "if len(unique_traps) < 2:\n",
    "    raise KeyError('not enough trap data for cancellation, only traps: '+str(unique_traps))\n",
    "\n",
    "# ONLY WITH 2 TRAP DATA!!!\n",
    "for name, grp in step7_results.groupby([\"mcycle\", \"position\"]):\n",
    "    print(\"mcycle, trap:\", name, \"positions\", grp.position.unique(), \"subcycles\", grp.subcycle.unique())\n",
    "\n",
    "    results = ratio_analysis.calc_ratios(grp, y=['nu_c'], yerr=['dnu_c'], groupbys=['cycle', 'subcycle'], identifier=\"trap\", ident_types=traps,\n",
    "            keep_columns=['mcycle', 'position', 'time_p', 'trap'], additional_identifiers=['ion', 'time_p', 'time'], mean_columns=[])\n",
    "        \n",
    "    step11_trap_ratio_data = step11_trap_ratio_data.append(results, ignore_index=True)\n",
    "\n",
    "step11_trap_ratio_data['masked'] = False\n",
    "display(step11_trap_ratio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step11_trap_ratio_data[\"Rminus\"] = step11_trap_ratio_data[\"ratio_nu_c\"] - 1.0\n",
    "\n",
    "step11_trap_ratio_data.to_csv(results_dir + \"step11_trap_ratio_data.csv\")\n",
    "step11_trap_ratio_data.to_csv(results_dir + \"step11_trap_ratio_data.txt\", sep=\"\\t\")\n",
    "\n",
    "fig = px.line(step11_trap_ratio_data, x=\"time_p\", y=\"Rminus\", error_y=\"dratio_nu_c\", facet_row='position', hover_data=['mcycle', 'cycle'])\n",
    "fig.update_yaxes(matches=None, showticklabels=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Interpolated Cancellation\n",
    "The trap ratios are interpolated to the same time. It works exacly the same as the interpolation in step 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For the interpolation we have to have averaged data.\n",
    "step12_cancel_interpol_results = pd.DataFrame()\n",
    "step12_avg_data = pd.DataFrame()\n",
    "for grpname, grpdata in step11_trap_ratio_data.groupby([\"mcycle\", \"position\"]):\n",
    "    print('mcycle, position', grpname)\n",
    "    avgdata = pd.DataFrame()\n",
    "    for subname, subgrpdata in grpdata.groupby(\"cycle\"):\n",
    "        avg = statistics.average_subsets(subgrpdata, groupby=[\"cycle\"], errortype=\"weighted\",\n",
    "                                              columns=[\"ratio_nu_c\", \"time_p\"], \n",
    "                                              dcolumns=[\"dratio_nu_c\", None],\n",
    "                                              masked=False)\n",
    "        avgdata = avgdata.append(avg)\n",
    "        \n",
    "    avgdata['masked'] = False\n",
    "    \n",
    "    # filter by min max boundaries for values, default: min_val: 1, max_val: 1e9:\n",
    "    avgdata = filtering.minmax_value(avgdata, val=\"ratio_nu_c\", min_val=0, max_val=100)\n",
    "\n",
    "    # apply autofilter 3-sigma condition: calc mean of values and std\n",
    "    # if value is outside of mean+-3*std, it is masked\n",
    "    avgdata = filtering.three_sigma(avgdata, val=\"ratio_nu_c\", err=\"dratio_nu_c\", undrift_xcolumn=\"time_p\", show=False)\n",
    "    #display(subset)\n",
    "\n",
    "    # apply autofilter sigma-size: if sigma of value is 3 time bigger\n",
    "    # then mean sigma, it is masked\n",
    "    avgdata = filtering.sigma_size(avgdata, err=\"dratio_nu_c\")\n",
    "       \n",
    "    step12_avg_data = step12_avg_data.append(avgdata)\n",
    "\n",
    "print(\"averaged\")\n",
    "#display(step12_avg_data)\n",
    "step12_avg_data = step12_avg_data.sort_values(\"time_p\")\n",
    "step12_avg_data.reset_index(drop=True, inplace=True)\n",
    "step12_interpolated_data = pd.DataFrame()\n",
    "step12_cancel_interpol_results = pd.DataFrame()\n",
    "positions = [\"position_1\",\"position_2\"] # in this position trap 2 is numerator (trap2/trap3)\n",
    "if not settings.get(\"invert\", True):\n",
    "    positions = positions[::-1]\n",
    "\n",
    "for grpname, grpdata in step12_avg_data.groupby([\"mcycle\"]):\n",
    "    print('mcycle', grpname)\n",
    "    reference, interpolate = 'position_1', 'position_2' # what was measured first in time? Thats the one we want to interpolate\n",
    "\n",
    "    interpolated_data = ratio_analysis.interpolate(grpdata, y=['ratio_nu_c'], yerr=['dratio_nu_c'], groupbys=['mcycle'], x=\"time_p\",\n",
    "                    identifier=\"position\", id_reference=reference, id_interpolate=interpolate,\n",
    "                    non_linear_uncertainty_per_second=0)\n",
    "        \n",
    "    step12_interpolated_data = step12_interpolated_data.append(interpolated_data)\n",
    "    \n",
    "    #if not invert:\n",
    "    #    positions = [\"position_1\",\"position_2\"]\n",
    "    #else:\n",
    "    #    positions = [\"position_2\",\"position_1\"]\n",
    "    \n",
    "    results = ratio_analysis.calc_ratios(interpolated_data, y=['ratio_nu_c'], yerr=['dratio_nu_c'], groupbys=['cycle', 'subcycle'], identifier=\"position\", ident_types=positions,\n",
    "            keep_columns=['mcycle', 'time_p', 'trap'], additional_identifiers=['ion_numer', 'ion_denom', 'time_p'], mean_columns=[])\n",
    "\n",
    "    step12_cancel_interpol_results = step12_cancel_interpol_results.append(results, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#display(step11_cancel_naive_results)\n",
    "\n",
    "# the 'ratio' calculated in the step before is acutally qmA^2 / qmB^2, so we have to take the sqrt (make sure you get the Error right!)\n",
    "step12_cancel_interpol_results[\"R\"] = np.sqrt( step12_cancel_interpol_results[\"ratio_ratio_nu_c\"] )\n",
    "step12_cancel_interpol_results[\"dR\"] = step12_cancel_interpol_results[\"dratio_ratio_nu_c\"]/step12_cancel_interpol_results[\"R\"]/2\n",
    "step12_cancel_interpol_results[\"Rminus\"] = step12_cancel_interpol_results[\"R\"] - 1    \n",
    "\n",
    "step12_cancel_interpol_results.to_csv(results_dir + \"step12_cancel_interpol_results.csv\")\n",
    "step12_cancel_interpol_results.to_csv(results_dir + \"step12_cancel_interpol_results.txt\", sep=\"\\t\")\n",
    "\n",
    "fig = px.line(step12_cancel_interpol_results, x=\"time_p\", y=\"Rminus\", error_y=\"dR\", hover_data=['mcycle', 'cycle'])\n",
    "fig.show()\n",
    "\n",
    "# TIP: it should be ion_numer_numer = ion_denom_denom and ion_numer_denom = ion_denom_numer (ending up ionA^2 / ionB^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"interpolated \": step9_interpolate_results,\n",
    "    \"poly all     \": step10_polyfit_best,\n",
    "    \"cancel interp\": step12_cancel_interpol_results,\n",
    "}\n",
    "\n",
    "visualization.compare(results, groupby=[\"mcycle\", \"trap\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13: Polyfit Cancellation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# you might want to do manual grouping\n",
    "temp_df = pd.DataFrame()\n",
    "step11_trap_ratio_data['masked'] = False # the ratio calc was already masked selective.\n",
    "for gname, grp in step11_trap_ratio_data.groupby(['mcycle']):\n",
    "    grp = polynom_fit.auto_group_subset(grp, 4, 4-1, sortby=[\"cycle\", \"time_p\"])\n",
    "    temp_df = temp_df.append(grp)\n",
    "\n",
    "temp_df.sort_values(['mcycle', 'time_p'], inplace=True)\n",
    "temp_df.reset_index(drop=True, inplace=True)\n",
    "#display(step7_results)\n",
    "\n",
    "visualization.manual_grouping_plot(temp_df, groupby=[\"mcycle\"], sets=\"position\", y='ratio_nu_c', yerr='dratio_nu_c', x=\"time_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step11_trap_ratio_data = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Rguess = step9_interpolate_results['R'].mean()\n",
    "\n",
    "step13_cancel_polyfit = pd.DataFrame()\n",
    "step13_cancel_polyfit_best_fits = pd.DataFrame()\n",
    "step13_cancel_polyfit_best = pd.DataFrame()\n",
    "step11_trap_ratio_data['ion'] = step11_trap_ratio_data['ion_numer']\n",
    "degrees = settings.get(\"polydegrees\", 'auto')\n",
    "polygrouping = settings.get(\"polygrouping\", 'auto')\n",
    "invert = settings.get(\"invert\", True)\n",
    "import time\n",
    "start = time.perf_counter()\n",
    "\n",
    "for idx, mc in enumerate(mcs):\n",
    "    print(\">>> mcycle:\", mc)\n",
    "    # get the subset\n",
    "    subset = step11_trap_ratio_data[ (step11_trap_ratio_data[\"mcycle\"] == mc) ]\n",
    "    if len(subset.index) < 3:\n",
    "        continue\n",
    "    \n",
    "    # get the testing parameters\n",
    "    if isinstance(polygrouping, str) and polygrouping == 'auto':\n",
    "        group_sizes = list(range(3, int(np.ceil(len(subset.cycle.unique())/4))+1)) # e.g. 24 data points->12 points per position->6/6 is the smallest splitting\n",
    "        group_sizes.append(0) # 0 stands for full main cycle\n",
    "    else:\n",
    "        group_sizes = polygrouping\n",
    "    if not group_sizes:\n",
    "        group_sizes = [3]\n",
    "    print(\"group sizes:\", group_sizes)        \n",
    "\n",
    "    if isinstance(degrees, str) and degrees == 'auto':\n",
    "        max_degree = min( [ len(subset.cycle.unique())-2, 15 ] ) # maximum degree 15 ( anything else is... too much(?) )\n",
    "        poly_degrees = list( range(1, max_degree) ) # e.g. full data set 24 data points - 2 for ratio and keeping one free\n",
    "    else:\n",
    "        poly_degrees = degrees\n",
    "    if not poly_degrees:\n",
    "        poly_degrees = [1]\n",
    "    print(\"polynom degrees:\", poly_degrees)\n",
    "\n",
    "    # fit with all the settings\n",
    "    subset, results, best_fits, best_results = polynom_fit.fit_sharedpoly_ratio(subset, Rguess, y=\"ratio_nu_c\", yerr=\"dratio_nu_c\", data_identifier=\"position\", \n",
    "                                           invert=invert, groupsize=group_sizes, degree=poly_degrees, mode='curvefit', \n",
    "                                           x=\"time_p\", keep_columns=[\"mcycle\", \"trap\"], bestfit=\"AICc\", bestgroupsize=\"chi2red\", show=False)\n",
    "\n",
    "    step13_cancel_polyfit = step13_cancel_polyfit.append(results, ignore_index=True)\n",
    "    step13_cancel_polyfit_best_fits = step13_cancel_polyfit_best_fits.append(best_fits, ignore_index=True)\n",
    "    step13_cancel_polyfit_best = step13_cancel_polyfit_best.append(best_results, ignore_index=True)\n",
    "\n",
    "step13_cancel_polyfit[\"Rfit\"] = step13_cancel_polyfit[\"R\"]\n",
    "step13_cancel_polyfit_best_fits[\"Rfit\"] = step13_cancel_polyfit_best_fits[\"R\"]\n",
    "step13_cancel_polyfit_best[\"Rfit\"] = step13_cancel_polyfit_best[\"R\"]\n",
    "step13_cancel_polyfit[\"dRfit\"] = step13_cancel_polyfit[\"dR\"]\n",
    "step13_cancel_polyfit_best_fits[\"dRfit\"] = step13_cancel_polyfit_best_fits[\"dR\"]\n",
    "step13_cancel_polyfit_best[\"dRfit\"] = step13_cancel_polyfit_best[\"dR\"]\n",
    "\n",
    "stop = time.perf_counter()\n",
    "print(\"time:\", stop-start)\n",
    "        \n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.options.display.float_format = '{: .12f}'.format\n",
    "display(step13_cancel_polyfit.head(10))\n",
    "display(step13_cancel_polyfit_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results\n",
    "\n",
    "polynom_fit.plot_best_results(step11_trap_ratio_data, step13_cancel_polyfit_best, groupby=['mcycle'], x='time_p', y='ratio_nu_c', yerr='dratio_nu_c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the 'ratio' calculated in the step before is acutally qmA^2 / qmB^2, so we have to take the sqrt (make sure you get the Error right!)\n",
    "step13_cancel_polyfit[\"R\"] = np.sqrt( step13_cancel_polyfit[\"Rfit\"] )\n",
    "step13_cancel_polyfit[\"dR\"] = step13_cancel_polyfit[\"dRfit\"]/step13_cancel_polyfit[\"R\"]/2\n",
    "step13_cancel_polyfit[\"Rminus\"] = step13_cancel_polyfit[\"R\"] - 1\n",
    "step13_cancel_polyfit_best_fits[\"R\"] = np.sqrt( step13_cancel_polyfit_best_fits[\"Rfit\"] )\n",
    "step13_cancel_polyfit_best_fits[\"dR\"] = step13_cancel_polyfit_best_fits[\"dRfit\"]/step13_cancel_polyfit_best_fits[\"R\"]/2\n",
    "step13_cancel_polyfit_best_fits[\"Rminus\"] = step13_cancel_polyfit_best_fits[\"R\"] - 1\n",
    "step13_cancel_polyfit_best[\"R\"] = np.sqrt( step13_cancel_polyfit_best[\"Rfit\"] )\n",
    "step13_cancel_polyfit_best[\"dR\"] = step13_cancel_polyfit_best[\"dRfit\"]/step13_cancel_polyfit_best[\"R\"]/2\n",
    "step13_cancel_polyfit_best[\"Rminus\"] = step13_cancel_polyfit_best[\"R\"] - 1\n",
    "\n",
    "display(step13_cancel_polyfit_best)\n",
    "\n",
    "\n",
    "step13_cancel_polyfit.to_csv(results_dir + \"step13_cancel_polyfit.csv\")\n",
    "step13_cancel_polyfit.to_csv(results_dir + \"step13_cancel_polyfit.txt\", sep=\"\\t\")\n",
    "step13_cancel_polyfit_best_fits.to_csv(results_dir + \"step13_cancel_polyfit_best_fits.csv\")\n",
    "step13_cancel_polyfit_best_fits.to_csv(results_dir + \"step13_cancel_polyfit_best_fits.txt\", sep=\"\\t\")\n",
    "step13_cancel_polyfit_best.to_csv(results_dir + \"step13_cancel_polyfit_best.csv\")\n",
    "\n",
    "step13_cancel_polyfit_best.to_csv(results_dir + \"step13_cancel_polyfit_best.txt\", sep=\"\\t\")\n",
    "\n",
    "fig = px.line(step13_cancel_polyfit_best, x=\"time_p\", y=\"Rminus\", error_y=\"dR\", hover_data=['mcycle', 'cycle_start'])\n",
    "fig.show()\n",
    "\n",
    "# TIP: it should be ion_numer_numer = ion_denom_denom and ion_numer_denom = ion_denom_numer (ending up ionA^2 / ionB^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results = {\n",
    "    \"interpolated\": step9_interpolate_results,\n",
    "    \"poly\": step10_polyfit_best,\n",
    "    \"cancel interp\": step12_cancel_interpol_results,\n",
    "    \"cancel poly\": step13_cancel_polyfit_best,\n",
    "}\n",
    "\n",
    "visualization.compare(results, timecol='time_p', groupby=[\"mcycle\", \"trap\"])\n",
    "visualization.allancompare(results, time='time_p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualization.compare(results, timecol='time_p', groupby=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"\"\"\n",
    "data_to_look_at = step8_naive_results_merged\n",
    "#data_to_look_at = step11_cancel_naive_results_merged\n",
    "#data_to_look_at = step7_results[ (step7_results['trap']==2) & (step7_results['position']=='position_2') & (step7_results['masked']==False) ]\n",
    "#ycol = 'R'\n",
    "ycol = 'nu_c'\n",
    "tcol = 'time_p'\n",
    "data_to_look_at.sort_values(tcol)\n",
    "\n",
    "data_to_look_at['epoch'] = data_to_look_at[tcol].astype(\"int64\")//1e9\n",
    "data_to_look_at['seconds'] = data_to_look_at['epoch'] - data_to_look_at['epoch'].min()\n",
    "statistics.allantest(data_to_look_at[ycol].to_numpy(), data_to_look_at.seconds.to_numpy())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more or less interesting data checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# linear fits of axial reference phase\n",
    "copyphases = nu_p_phases.copy()\n",
    "copyphases['epoch'] = copyphases['time'].astype('int64')//1e9\n",
    "\n",
    "slopes = {\n",
    "    2: [],\n",
    "    3: []\n",
    "}\n",
    "for grpname, grp in copyphases.groupby(['trap', 'position', 'acc_time']):\n",
    "    if grpname[-1] != copyphases.acc_time.min():\n",
    "        continue\n",
    "    grp_new = phase_analysis.unwrap_dset(grp, column=\"phase\", drift_unwrap=True, drift_pi_span=1.5, timesort=True, reverse=False, show=False)\n",
    "    phases = grp_new['phase'].to_numpy()\n",
    "    times = grp_new['epoch'].to_numpy()\n",
    "    \n",
    "    opt = np.polyfit(times,phases, 1)\n",
    "    print(opt)\n",
    "    slopes[grpname[0]].append(opt[0])\n",
    "    plt.plot(times, phases)\n",
    "    plt.plot(times, opt[1] + opt[0]*times)\n",
    "    plt.show()\n",
    "    \n",
    "print(slopes)\n",
    "print(np.mean(slopes[2])/np.mean(slopes[3]))\n",
    "print(732536.1/501554.07)\n",
    "print((np.mean(slopes[2])/np.mean(slopes[3]) - 732536.1/501554.07)/(732536.1/501554.07))\n",
    "print((np.mean(slopes[2])/np.mean(slopes[3]) - 732536.1/501554.07))\n",
    "print(724000/475000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "test = step9_interpolate_results[step9_interpolate_results['trap']==3]\n",
    "print(test.R.std(), test.dR.mean(), test.dR.mean()/test.R.mean())\n",
    "\n",
    "R = test.R.to_numpy()\n",
    "time = test.time_p.astype(\"int64\")//1e9\n",
    "time = time.to_numpy()\n",
    "time = time - time.min()\n",
    "time = time/60/60\n",
    "\n",
    "blubb = visualization.allanvariance(R/np.mean(R), time, plot=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ion_num = step8_naive_results_merged['ion_numer'].unique()[0]\n",
    "ion_den = step8_naive_results_merged['ion_denom'].unique()[0]\n",
    "from uncertainties import ufloat\n",
    "\n",
    "results = {\n",
    "    \"interpolated \": step9_interpolate_results,\n",
    "    \"poly all     \": step10_polyfit_best,\n",
    "}\n",
    "\n",
    "omegac_ref = omegac_ioi = omegap_ref = omegap_ioi = omegam_ref = omegam_ioi = None\n",
    "exc_radius = ufloat(20, 5)*1e-6\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "dy = []\n",
    "for trap, grp in step7_results.groupby(['trap']):\n",
    "\n",
    "    pos_ref = grp[grp['ion'] == ion_den]\n",
    "    pos_ioi = grp[grp['ion'] == ion_num]\n",
    "        \n",
    "    omegac_ref = ufloat(pos_ref.nu_c.mean()*2*np.pi, 0.1)\n",
    "    omegap_ref = ufloat(pos_ref.nu_p.mean()*2*np.pi, 1)\n",
    "    omegam_ref = ufloat(pos_ref.nu_m.mean()*2*np.pi, 1)\n",
    "            \n",
    "    omegac_ioi = ufloat(pos_ioi.nu_c.mean()*2*np.pi, 0.1)\n",
    "    omegap_ioi = ufloat(pos_ioi.nu_p.mean()*2*np.pi, 1)\n",
    "    omegam_ioi = ufloat(pos_ioi.nu_m.mean()*2*np.pi, 1)\n",
    "    \n",
    "\n",
    "    for name, data in results.items():\n",
    "        \n",
    "        tdata = data[data['trap'] == trap]\n",
    "        ratios = tdata.R.to_numpy()\n",
    "        dratios = tdata.dR.to_numpy()\n",
    "        R, err_in, err_out, chi2red = statistics.complete_mean_and_error(ratios, dvalue = dratios)\n",
    "        dR = max(err_in, err_out)\n",
    "        uR = ufloat(R, dR)\n",
    "        #print((uR-1)*1e12)\n",
    "        if uR.n > 1:\n",
    "            uR = 1/uR\n",
    "        \n",
    "        Eb_ref = ufloat(31529.1, 8.8)\n",
    "        Eb_ioi = Eb_ref\n",
    "        eV, Q, nRatio = systematics.neutral_mass(uR.n, uR.s, ion_den, ion_num, Rcompare=None,\n",
    "                        Eb_ref=Eb_ref, Eb_ioi=Eb_ioi,\n",
    "                        sys_on=True, exc_radius=exc_radius, show=False,\n",
    "                         omegac_ref=omegac_ref, omegac_ioi=omegac_ioi, omegap_ref=omegap_ref, omegap_ioi=omegap_ioi,\n",
    "                         omegam_ref=omegam_ref, omegam_ioi=omegam_ioi)\n",
    "        \n",
    "        #print((nRatio-1)*1e12)\n",
    "\n",
    "        print(name, trap, eV, Q, nRatio, 1/nRatio, nRatio.s/nRatio.n, uR, 1/uR, uR.s/uR.n)\n",
    "        x.append(name.strip()+'_'+str(trap))\n",
    "        y.append(nRatio.n)\n",
    "        dy.append(nRatio.s)\n",
    "\n",
    "uRn_rana = ybrana.make_ybratio(ion_num[:-3], ion_den[:-3])\n",
    "uRn_rana = 1/uRn_rana\n",
    "print(uRn_rana)\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "plt.errorbar(['rana'], [uRn_rana.n], [uRn_rana.s] )\n",
    "plt.errorbar(x, y, dy)\n",
    "plt.show()\n",
    "#uR = 1/uR\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
